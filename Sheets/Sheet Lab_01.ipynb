{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01 Linear Regression.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3dU7qsmMCDN"
      },
      "source": [
        "# Lab 1: Linear Regression\r\n",
        "\r\n",
        "Welcome to the second lab session of the Machine Learning Course. In this session, let's look at a fundamental topic in Machine Learning - Linear Regression\r\n",
        "\r\n",
        "Before we start, let us see what 'Regression' means.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0vsYULgM_x2"
      },
      "source": [
        "### Regression\r\n",
        "Regression is a statistical concept, which refers to finding relationships between different quantities. This lies at the core of Data Sciences. Finding how different types of data are related are used to find meaningful conclusions and decisions. \r\n",
        "\r\n",
        "In the context of Machine Learning, Regression refers to the prediction of a *continuous* mathematical real-numbered variable as the output, when the model is given some inputs, in the form of variables. This output value is predicted by observing patterns in the data you provide to it. \r\n",
        "\r\n",
        "For example: look at this image below and observe any pattern that you can interpret visually. \r\n",
        "\r\n",
        "![image](https://drive.google.com/uc?id=1v3vc-cqwrMekvjtT4XLY2XSNzbH51Rwf)\r\n",
        "\r\n",
        "This [chart](https://www.worldometers.info/coronavirus/country/us/) shows the daily Covid-19 Cases in the United States. Do you see some trends? You would observe that the number of daily cases rise and fall and rise again, almost like a wave. \r\n",
        "\r\n",
        "Hospitals need to prepare resources, such as hospital beds, ventilators, oxygen supply, and even testing kits well in advance. Thus, an estimate of the expected number is surely helpful. \r\n",
        "\r\n",
        "Observe the part towards the end of the curve on the right. The chart tells us, that everyday since 29 December 2019, the number of cases have crossed 200,000. Moreover, since December 1, till Jan 15, the number of cases have crossed 200,000 on all days except some 7 odd days. But to compensate this, the number of new cases cases have even risen beyond 300,000 on one day, and more than 250,000 on many days. \r\n",
        "\r\n",
        "If you were to predict the  number of cases on the following day, it would be very reasonable to the number of Covid-19 positive patients to be above 200,000, because thats the trend. Ofcourse, you never know, the number may be lower or even extremely high - It may even cross 400,000. But *chances* are the number will be in this ballpark. This is what regression means - to observe the patterns in the data and *predict* a value with high confidence. Here, given the input, such as the number of new cases in the past month, the number of tests carried out, and the history of the events of the past few days (for example, public events that may have acted as super-spreader events), we predict how many cases there will be in the future. Infact, this is what statisticians do - they predict how worse the pandemic can get, and when the curve is expected to flatten, using these techniques.\r\n",
        "\r\n",
        "One important concept to understand is - that we can only **predict** our results, with a probability (or, in other words, with high confidence), and so, there is always a chance that our prediction is wrong, or off by a large margin. Our job is to give the best possible results, based on the data that we have been provided with. And a larger probability (ie, higher confidence) is desirable, because we can be more sure that our results will be accurate (or close to the actual value).  \r\n",
        "\r\n",
        "Let us begin with our discussion on Linear Regression!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faQ_eIF4OnCV"
      },
      "source": [
        "#### Linear Regression\r\n",
        "\r\n",
        "Given some data `x`, the results of which we know, and represent as `y` (which may be refered to as the target value, or the ground truth value) (keep in mind, the target values are not predictions - they are actual values which were factually observed in the real world), our simplest approach can be to map a function `y=f(x)` that maps the relation between the inputs (`x`) and the output (`y`). \r\n",
        "\r\n",
        "The simplest function that can be made to \"learn\" these relations and patterns is a linear function. Given an input `x`, the output will be given as:\r\n",
        "\r\n",
        "```\r\n",
        " f(x) = w.x + b  #this is our model. '.' represents multiplication\r\n",
        " pred = f(x)  #pred is the prediction of the output, given some input x\r\n",
        "```\r\n",
        "The coefficient of `x`, ie, **`w`**, and the intercept term **`b`** are the *parameters* of our linear function. If the linear function is designed properly, meaning, if `w` and `b` are given proper values, then `pred` can be closer to the actual output y. Note that our parameters are real valued numbers.\r\n",
        "\r\n",
        "This is essentially learning the patterns of the data. So now, when you want to predict the output, of which you **do not** know the ground truth output (for example, future prediction), the model will predict the output based on the data we had provided to it. This is because, we assume, that all data shows a common pattern. Since, we have learned that pattern by adjusting our parameters, our predictions are expected to give good results. \r\n",
        "\r\n",
        "Above is shown the linear regression technique for ONE independent variable (x) which is to be mapped to ONE dependent variable (y). You may have multiple dependent variables (x1, x2, x3 and so on), which is to be mapped to the dependent variable (y) , and in that case, the model looks like:\r\n",
        "```\r\n",
        "f(x1,x2,x3 ...) = w1.x1 + w2.x2 + w3.x3 + .... + b\r\n",
        "pred = f(x1,x2,x3 ...)\r\n",
        "```\r\n",
        "![image](https://drive.google.com/uc?id=10-K2YfkYT8O24I5eS9xWIHL6H6Xf3pbj)\r\n",
        "\r\n",
        "In this picture, we fit a linear regression model to the data. Now for an unseen data, the model would predict a value that lies on this line. This is a fairly accurate representation of the data, since we observe that all the data more or less lies alone a straight line.\r\n",
        "\r\n",
        "As an example, let us take a look at the price of the Google stock in the New York Stock Exchange, over the course of many years.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJXSE2tV2NFc"
      },
      "source": [
        "##### **Case Study: The New York Stock Exchange**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTVlOpIesZUp"
      },
      "source": [
        "###### **Setting up our Data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiJVwuuH2Syz"
      },
      "source": [
        "Let us begin by downloading the [NYSE Dataset](https://www.kaggle.com/dgawlik/nyse), which contains details of the  . If you click on the hyperlink, you will be redirected to a website called *kaggle.com*. This website is one of the most famous websites for Data Science Practitioners. It provides access to many datasets, both small-scale and large-scale, and a platform for practitioners to discuss their solutions, share ideas, and even compete. Many competetions are even paid, meaning the winner of the competition wins a price money of thousands of US dollars, sometimes even in millions! Infact, many game-changing Machine Learning research papers have resulted from kaggle competitions. And many top *kagglers* (as they are called), called as *kaggle grandmasters*  are known to have been offered high-paid positions as Data Scientists at top tech-companies and research institutes. \r\n",
        "\r\n",
        "To download datasets from Kaggle, we need an account. **So head over to the website and make an account**! You can simply signup using your google account, without having to go through the entire procedure!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqAOoJI5W1tn"
      },
      "source": [
        "Kaggle provides us with an API (a function that saves us from a lot of technicalities) to pull datasets from the website. For that, we need to download a file which contains the mechanism to download datasets over the cloud. This file is goes by the name `kaggle.json`, and is unique to each kaggle account (This is done so that Kaggle knows who is downloading the dataset, because when a lot of data belongs to companies, which have their own legal requirements. So this is a method to hold accountability against any user who misuses the data. We don't need to worry about it!) \r\n",
        "\r\n",
        "So...\r\n",
        "\r\n",
        "* Head over to www.kaggle.com \r\n",
        "* On the top right corner, click on your profile photo. Then go to *My Profile*\r\n",
        "* Click on *Account* on the top menu bar.\r\n",
        "* Click on *Create new API Token*. A file named `kaggle.json` will be downloaded!    \r\n",
        "\r\n",
        "Ideally, the API expects the file to be located in your computer at the location `~/.kaggle/kaggle.json`. \r\n",
        "\r\n",
        "(So if you're using your PC rather than this notebook, you can simply create a folder named `.kaggle` in your root directory, and put the `kaggle.json` file in that directory)\r\n",
        "\r\n",
        "For those who are using this notebook, this poses a problem! We are not running this notebook using our own PC's CPU/GPU, but rather running this notebook on a PC on a remote Google server, and we don't have the interface to the PC on which this notebook is running. We only have access to this notebook. But, below we have provided you with a peice of code, that lets you upload the `kaggle.json` file from your computer, and send it to the remote PC on which we're working. The source of the code can be found [here](https://colab.research.google.com/github/corrieann/kaggle/blob/master/kaggle_api_in_colab.ipynb#scrollTo=0HtGf0HEXEa5). \r\n",
        "\r\n",
        "Upload the `kaggle.json` file using the `Choose Files` button after running the cell below. Make sure the name of the file is nothing else other than `kaggle.json`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KAU678fP4aLU"
      },
      "source": [
        "#Code to upload kaggle.json from local PC to the remote server\r\n",
        "from google.colab import files\r\n",
        "\r\n",
        "uploaded = files.upload()\r\n",
        "\r\n",
        "for fn in uploaded.keys():\r\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\r\n",
        "      name=fn, length=len(uploaded[fn])))\r\n",
        "  \r\n",
        "# Then move kaggle.json into the folder where the API expects to find it.\r\n",
        "!mkdir -p ~/.kaggle/ && mv kaggle.json ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KGoAQ7EeqYN"
      },
      "source": [
        "Then, to download the dataset, we go to the [dataset page](https://www.kaggle.com/dgawlik/nyse), click on the three vertical dots on the right, and click on *Copy API command*.\r\n",
        "\r\n",
        "Now paste this command in the cell below. But to run this code, we need to add a `!` at the beginning of the command. This is because this code is not a python code, but a terminal code. The `!` tells that Jupyter Notebook that the code written following this, is a terminal code, and not in python. We've already written this for you..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwtYFMDaLReM"
      },
      "source": [
        "!kaggle datasets download -d dgawlik/nyse"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ma2x_keugS_a"
      },
      "source": [
        "Next, unzip the dataset file (which is downloaded in the form of a zip file). We also move the contents of the file to a new folder, which we name \"nyse_dataset\". \r\n",
        "\r\n",
        "Once we unzip the dataset, we take a look at the dataset folder using the `ls` terminal command"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9VPM05fLgaU"
      },
      "source": [
        "!mkdir nyse_dataset\r\n",
        "#mkdir is the terminal command to make a new folder with a specific name (which here, is \"nyse_dataset\")\r\n",
        "!unzip nyse.zip -d nyse_dataset #unzip the nyse.zip file and move the contents to the folder named \"nyse_dataset\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XnSSXSruLkAb"
      },
      "source": [
        "%cd /content/nyse_dataset \r\n",
        "!ls\r\n",
        "#These are the files inside the downloaded dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KH4Z7-6rGHQ"
      },
      "source": [
        "That's it regarding the setup of our dataset! The process may seem slightly overwhelming at first, but its really not. Its just 3 steps - \r\n",
        "1. Make sure your API key (the json file) is present. If you're using your local PC to run this code, you dont even have to do this again and again.\r\n",
        "2. Download the dataset using the command that Kaggle provides you with\r\n",
        "3. Uzip it and visualize it!\r\n",
        "\r\n",
        "It becomes almost effortless over time. So do not worry if you cannot wrap your head around everything at once. The important idea is to understand the workflow.\r\n",
        "\r\n",
        "Now lets actually start processing our data!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwQiPXtwsfta"
      },
      "source": [
        "###### **Working with our data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OumkUoAzl9LC"
      },
      "source": [
        "We will be carrying out our analysis on the `prices-split-adjusted.csv` file. Notice that our file is a `.csv` file, which stands for \"comma separated values\". Its similar to an Excel file containing data. If you open a csv file using any text editor, you would find data in a row separated with commas, and each line acting as a whole row.\r\n",
        "\r\n",
        "`pandas` is one of the most widely used libraries in python to handle csv files. We will create a \"dataframe\" using this library, which is nothing but the data represented in the form of a table. This can be used to extract certain values in the data. \r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8x7w94F2NZHm"
      },
      "source": [
        "import pandas as pd # we call the pandas library and give it an alias (pd), which we will use as a shortcut to call pandas\r\n",
        "dataframe = pd.read_csv( 'prices-split-adjusted.csv', parse_dates=['date']) #store the dataset in the prices-split-adjusted.csv in the variable called dataframe\r\n",
        "dataframe.head() #.head() is a method to print the first 5 rows of the dataframe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tgO81fooqkI"
      },
      "source": [
        "Let us understand this dataset. The first column of the dataframe is the date on which the stock prices are recorded. According to the dataset description on the kaggle page, the stocks are recorded from 2010 to 2016. \r\n",
        "The second column is the Stock Exchange symbol of the company. To find all the companies in the dataset, we use the `unique()` method, which finds all unique values in a column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7kY-U1DdNyUP"
      },
      "source": [
        "dataframe['symbol'].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWfexrTdsrf3"
      },
      "source": [
        "We'll work with the dataset of only one company - Google (Now called the Alphabet Inc Company) (the symbol of which is 'GOOG'.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmBNjVX1qza-"
      },
      "source": [
        "google_df = dataframe[dataframe['symbol']=='GOOG'].set_index('date')\r\n",
        "google_df.head() #notice, the dates act as the indices of the dataframe, ie the key that we can use to refer to a particular row"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Y-nR5EOtJz0"
      },
      "source": [
        "len(google_df) #let us check how many rows of data do we have"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOtNgLraED7O"
      },
      "source": [
        "#additional information: \r\n",
        "#to extract one row of data, say of the date 2010-01-04, we use the loc method:\r\n",
        "google_df.loc['2010-01-04']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRHyWIPitSD8"
      },
      "source": [
        "The remaining of the columns are details of the stock price and company, such as the opening price on any day, the closing price, the highest and lowest stock price on any day, and the volumes of trades done on a day. We are concerned with the closing price of the google stock on each of these days. We are given the stock prices between 2010 and 2016. **Let us try to find what the stock price may be in the future. We will find the stock price at the end of each year from 2017 through 2020, and also try to predict what the stock price may be at the end of 2025, if the stock were to increase in value at the same rate.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvrjmnr16dph"
      },
      "source": [
        "Let us start by visualizing the data. We use the matplotlib library to plot our data. It is The most widely used python library to plot and visulaize data. Again, we'll be using an alias (plt) to avoid writing the entire phrase again and again."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xJtWTe0tMSK"
      },
      "source": [
        "import matplotlib.pyplot as plt\r\n",
        "plt.figure(figsize=(15, 7))\r\n",
        "plt.plot(google_df.close) #the x axis is the date, the y axis is the closing price of the stock on any given day"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dz85wmqf-Yy6"
      },
      "source": [
        "Can you guess which variable is the dependent variable and which is independent?\r\n",
        "\r\n",
        "The time is the independent variable, and the stock price is the dependent variable. Let us put them in the variables `x` (for independent variable), and `y` (for dependent variable). But how do we use the date as an input? Remember, all values need to real numbers.\r\n",
        "\r\n",
        "One approach is to use the number of days since the first day that is recorded in the dataset. Let us use this as the independent variable. We will make two different lists for `x` and `y` respectively. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdn0sZUv2ApZ"
      },
      "source": [
        "first_day = google_df.index.min() #remember, dates are indices to the dataframe\r\n",
        "first_day #this is the day relative to which we'll calculate the number of days passed."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vuiAspqDxIO"
      },
      "source": [
        "This refers to a specific time - 4 January 2010, at 00:00:00 hrs\r\n",
        "\r\n",
        "Let us define a function that takes in a date from the dataframe, and calculates the number of days since this date"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40Xd2TxY2KDt"
      },
      "source": [
        "def calculate_days_since(date,reference=first_day): return (date-reference).days \r\n",
        "\r\n",
        "#'date minus reference' would return the difference in terms of the exact time elapsed, till the exact milliseconds.\r\n",
        "#the .days method ignores everything except the number of days, which is what we require!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_exMutxIGxx"
      },
      "source": [
        "You might be wondering? how can we subtract two dates (date and reference)? \r\n",
        "Actually, go back above, and notice the statement where we read the csv file. \r\n",
        "there's an argument that says parse_dates: which tells pandas to not consider the column 'dates' as a string, but as meaningful objects.\r\n",
        "By writing this argument, pandas now knows that these strings are dates, and provides us with methods to handle them. One such is that, we \r\n",
        "can subtract two dates to get the difference. \r\n",
        "\r\n",
        "Internally, all dates are converted into objects of another python library, called the datetime.\r\n",
        "So if we are to pass a date to this function, we need to first convert it to a datetime object. So let us import that library."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmXHyk0m2gY3"
      },
      "source": [
        "from datetime import datetime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOP4xR6wIlTr"
      },
      "source": [
        "Let us look at if the function we defined above works. Lets see, how many days have passed between the reference date (4 Jan 2010) and 1 Jan 2021. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKfQRhEWIKhx"
      },
      "source": [
        "calculate_days_since(datetime(2021,1,1)) #The syntax of datetime is datetime(year, month, day)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4l_cf8iqJs1V"
      },
      "source": [
        "Yes, that works perfectly. If it doesn't, look if the syntax is correct (including, if brackets are opened and closed properly, and if the inputs are correct.\r\n",
        "\r\n",
        "Let us, finally, build our dataset (meaning, our `x` and `y`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ju74wx34NOTm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSpEYOrMIfJ7"
      },
      "source": [
        "x=list(map(calculate_days_since,google_df.index))\r\n",
        "x[:10] #seeing the first 10 items of our x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4V9KLgZ7Kh_K"
      },
      "source": [
        "y=list(google_df.close)\r\n",
        "y[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUGbXZmSL3m2"
      },
      "source": [
        "assert len(x)==len(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmj695RLMWvf"
      },
      "source": [
        "There's some python jargon here. Let us look at these. Everytime we come across a new, important concept in Python, we'll write a small note with the heading **\"LESSONS IN PYTHON\"** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvP-QqC0L-lC"
      },
      "source": [
        "\r\n",
        "---\r\n",
        "---\r\n",
        "### **LESSONS IN PYTHON**\r\n",
        "\r\n",
        "1. \r\n",
        "```\r\n",
        "x=list(map(calculate_days_since,google_df.index))\r\n",
        "```\r\n",
        "map is a function that can be used to apply a function over every value in a collection of values. Here, we wish to apply the `calculate_days_since` function over every value in the collection of dates, which is given by `google_df.index` (remember, the dates are the indices of the dataframe, and the .index method returns a collection of all indices). \r\n",
        "\r\n",
        " So, this function takes values from `google_df.index`, and pass it through the `calculate_days_since` function, and returns a collection of the values as `map` object. Obviously, this `map` object is of no use to us, we want the result to be in the form of a list. So we pass the result of the map function to the function `list()`, which returns a list of the values.\r\n",
        "\r\n",
        "2. ```\r\n",
        "assert len(x)==len(y)\r\n",
        "```\r\n",
        "\r\n",
        " assert is a sleek method in python to check a condition. If the condition is satisfied, it does * nothing *. If the condition is not satisfied, an error is raised. This immediately aborts all functionality, and can be used as an effective debugging tool, to find out where exactly in the code an error exists. And if no error exists, this method does not interfere with the functionality of the code.\r\n",
        "Here, we are making sure that the number of items in our dependent variable y is the same as the number of items in our independent variable x. So if no output is given, or no action takes place, it means that indeed, the condition is satisfied, and we're good to go!\r\n",
        "\r\n",
        "---\r\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EasZ2sbqSdSa"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOx-ThxnSeE9"
      },
      "source": [
        "Let us visualize the data using the `plt.plot(x_axis,y_axis)`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZ0kgzZUL-Mn"
      },
      "source": [
        "plt.plot(x,y)\r\n",
        "plt.ylabel('Stock price (closing)');\r\n",
        "plt.xlabel('Days since 4 Jan 2010');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhuR1aDSVT4w"
      },
      "source": [
        "## The Linear Regression Model\r\n",
        "We will look at two methods to build a Linear Regression Method:\r\n",
        "1. By using a direct formula.\r\n",
        "2. The Gradient Descent.\r\n",
        "\r\n",
        "The idea of Linear Regression is to make sure that our predictions are as close to the target value as possible. To ensure this, we use the method of Least squares. In other words, our objective is: Minimize the difference between the predictions and the ground truth. Since we do not care about the sign of the difference, we square the difference, in order to ensure we always get a positive value. \r\n",
        "\r\n",
        "Hence our goal can be written as:\r\n",
        "\r\n",
        "Minimize $\\frac{1}{n} \\sum_1^n(pred - y)^2$. \r\n",
        "\r\n",
        "We call this the loss function. Smaller the value of the loss function, the better the model is considered. It is nothing but the *average sum of squared errors*. \r\n",
        "\r\n",
        "### Method 1: Direct Formula:\r\n",
        "Given some input x and output y, you can directly calculating the line of best fit using the formula described below. :\r\n",
        "(Disclaimer: You Don't need to  memorize these formulae!)\r\n",
        "\r\n",
        "$ y= w_{1}x_1 + w_2x_2 + .... + w_k x_k + b$\r\n",
        "\r\n",
        "$w_i =  \\frac{\\sum_1^n(x_i - \\bar{x_i})(y - \\bar{y})}{\\sum_1^n(x_i - \\bar{x_i})}$\r\n",
        "\r\n",
        "$b= \\frac{1}{n} \\left \\{\\sum_{1}^{n} y - \\sum_{i=1}^{k} w_i (\\sum_{1}^{n}x_i)  \\right \\}$\r\n",
        "\r\n",
        "Where n is the number of training points, k is the number of independent variables, and all other notations have standard mathematical meanings. These calculations come from solving a differential equation in order to minimize the square of the difference.  \r\n",
        "\r\n",
        "This seems quite confusing. Fortunately, there are many python libraries out there that do this calculation for us. One such library is the `scikitlearn` library. Let us import that library."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXaavGEpSa33"
      },
      "source": [
        "from sklearn.linear_model import LinearRegression"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imjTvPsAy3aD"
      },
      "source": [
        "[Here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) is the documentation of the function. The `fit()` method is used to find the parameters of the model (w and b). However, according to the documentation, the model expects each data point to be a separate list, and all these lists to be contained in one massive list.  \r\n",
        "`[[i] for i in x]` does that for us. This method of making a list is called list comprehensions. Basically, for every value in x, which we denote as `i`, put a list containing i (`[i]`) inside the list. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqKrAEsPvuBB"
      },
      "source": [
        "modified_x = [[i] for i in x]\r\n",
        "print(modified_x[:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eqzTHXVN07LT"
      },
      "source": [
        "reg=LinearRegression().fit(modified_x,y) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxzfqMoB1GQ5"
      },
      "source": [
        "The weights `w` are stored in the `coef_` method of the `reg` object, and the intercept term `b` is stored in the `intercept_` method. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5sxPcoExNx3"
      },
      "source": [
        "w,b = reg.coef_ , reg.intercept_\r\n",
        "w,b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71iONcml1bHS"
      },
      "source": [
        "### Let us see how good this performs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDLmOqdv2kTn"
      },
      "source": [
        "#we define a function that predicts the values according to this model\r\n",
        "def lin_model(x): return x*w + b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMcnld0qwJkN"
      },
      "source": [
        "plt.plot(x,y)\r\n",
        "plt.plot(x,[lin_model(o) for o in x])\r\n",
        "plt.ylabel('Stock price (closing)');\r\n",
        "plt.xlabel('Days since 4 Jan 2010');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIZIaKjv1x7r"
      },
      "source": [
        "Looks pretty accurate! Visually too, you can see that the line fits pretty well within the data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4wEStFOw0pF"
      },
      "source": [
        "day=datetime(2017,12,31)\r\n",
        "days_elapsed= calculate_days_since(day)\r\n",
        "days_elapsed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFOONh9K2eX_"
      },
      "source": [
        "lin_model(days_elapsed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bj9U33bY4jQ8"
      },
      "source": [
        "### Method 2: Gradient Descent\r\n",
        "Gradient Descent is an algorithm which iteratively updates the values of the parameters until a goal is met, using derivatives. In our case, the goal is to minimize the loss function as we mentioned above. \r\n",
        "\r\n",
        "The way Gradient Descent works is by minimization of the loss function through the concept of gradients. Below is a dummy example:\r\n",
        "\r\n",
        "Consider a function that needs to be reduced. Let this be a quadratic function for the sake of simplicity. Our job is to find the value of x (on the x axis), that makes y (the independent variable), minimum!\r\n",
        "\r\n",
        "![image](https://drive.google.com/uc?id=1aT-hkr91lj6Gj8exxIzOZ0X7_QHOyhVP)\r\n",
        "\r\n",
        "Suppose you start from a random point on this graph, say (x,y) = (4,16). How do we, from here, proceed towards the *minima* of this graph?\r\n",
        "\r\n",
        "![image](https://drive.google.com/uc?id=1Rd3i0k69mG8x4ChHIyX0Qi5_9KwZBF6T)\r\n",
        "\r\n",
        "Gradients provide the answer! Gradients, if you don't know, is the slope of the curve. You must have studied about the method to calculate the gradient - *rise over run*. If you do not know what gradients are, head over to Khan Academy. [Here](https://www.youtube.com/watch?v=tIpKfDc295M) and [here](https://www.youtube.com/watch?v=_-02ze7tf08) are some videos to get you started!\r\n",
        "\r\n",
        "If you now have some idea about gradients, let us proceed with our discussion on gradients. Gradients are real valued numbers, which represent how the values of the graph are changing. \r\n",
        "\r\n",
        "Let us start by assigning a totally random value to our parameter x. Now, at the point (4,16) (the red point), can you guess whether the gradient of y with respect to x, is positive or negative? \r\n",
        "\r\n",
        "It is positive! Infact, if you know how to calculate the gradient of a quadratic function $f(x)=x^2$, it is simply $f'(x) = 2x$. So at x=4, the gradient is simply +8.\r\n",
        "\r\n",
        "**Gradient Descent continuously updates the parameter (x) by a value opposite to the sign of the gradient of the dependent variable(y) with respect to the parameter(x).**\r\n",
        "\r\n",
        "If the gradient of y with respect to x is positive, update x with a negative number. (Meaning, **Reduce** the value of x by a certain number.)\r\n",
        "\r\n",
        "If the gradient with respect to the parameter(x) is negative, increase the parameter(x). If you continuously update the parameters using this method, you will reach to the minima of the function. \r\n",
        "\r\n",
        "![image](https://drive.google.com/uc?id=1LjqBwdupEE7RoT1ogvYKVvoaVT6FVKzr)\r\n",
        "\r\n",
        "*Our parameter is continuously updated in steps that move towards the mimima. x goes from 4 to 3 to 2.2 to 1.5 and so on, until it reaches 0*\r\n",
        "\r\n",
        "How do we represent this practically?\r\n",
        "```\r\n",
        "#Step 1: calculate gradients\r\n",
        "gradient(x) = 2*x      #(gradient of the quadratic function x**2 is 2*x)\r\n",
        "\r\n",
        "#Step 2: update the parameters\r\n",
        "x -= step_size*gradient(x)      #x-=number is the same as x=x-number.\r\n",
        "```\r\n",
        "Let us understand the weight update step.\r\n",
        "You would understand why we are subtracting from x. This is because we are taking a step of the opposite sign of the gradient. If the gradient is positive, x increases by `step_size*gradient(x)`, and if negative, x decreases by the magnitide `step_size*gradient(x)`. \r\n",
        "\r\n",
        "What is step_size?\r\n",
        "Step_size is a small positive number, which lets us control **how big a parameter update takes place**. When we study Neural Networks, you will see why we need to control the size of the step. But you can get a basic intuition about the idea - if the step_size is too small, it'll take forever to reach the minima. If the step_size is too large, you may overshoot the minima. Meaning, for example, if you're currently at x=0.1, you need to update the parameter by 0.1 to reach x=0. But because of the high learning rate, you can only take a step bigger than this, and thus never reach the minima. The step_size is also known as the **learning rate** in Machine Learning. So keep this term in mind when we further study Gradient Descent, predominantly during Neural Networks! \r\n",
        "\r\n",
        "What happens when we finally reach x=0. We know that x=0 is the minima of this curve. But also notice, that the *gradient with respect to x is also 0*. So the weight update step does not update the parameter, and the value remains stagnant. At this point, we have fulfilled our goal! Notice how we started from a random value and still reached the minima using this algorithm!\r\n",
        " \r\n",
        " ---\r\n",
        "\r\n",
        "So how would Gradient Descent look like for our problem? What would be our model function (y)? and what would be the independent function (x)?\r\n",
        "\r\n",
        "y is the function which needs to be minimized, so this would be our Loss Function (the average sum of squared error between the target and model predictions). x would be our parameters which need to updated such that y (the dependent variable) is minimum. So we'll be running the gradient descent separately for each of the parameters (w1,w2,w3....,wk,b)\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izZ7qcYT8FzH"
      },
      "source": [
        "### **Implementing Gradient Descent**\r\n",
        "To implement this library, we'll be using PyTorch, a framework built on top of Python, which is known for efficiently handling Deep Learning (ie, the Machine Learning concerned with Neural Networks). The reason we are using PyTorch, is because PyTorch can calculate the gradients for us, without us having to tell the mathematical formula for the gradient. You can calculate the gradients for **ANY** function, however complex, and we don't even have to know how to calculate the gradient. It may seem bizarre at first - you may ask, how would pytorch know what our model, or loss function is! And so, how would it know how to calculate the gradients? Does it know the gradients of all functions in this universe?\r\n",
        "We will see how this works. This mechanism is called *autograd*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRkwuLunAEkA"
      },
      "source": [
        "#let us start by importing the PyTorch library, which is denoted by torch\r\n",
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFhSfzVI-9DP"
      },
      "source": [
        "We have 2 parameters, `w` and `b`, that we need to find values of, such that predictions are such that the loss function is minimum. (This sentence summarizes the entire concept of Linear Regression, so make sure you go through it again and understand it well). As we mentioned before, we will first start with random values for `w` and `b`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xT61_4Gw2zZf"
      },
      "source": [
        "params=torch.randn(2).requires_grad_() #initialize w and b with a random value of size 1. This is the dimension sizes.\r\n",
        "w,b=params\r\n",
        "w,b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGAgjD35C3Xd"
      },
      "source": [
        "Notice that these parameters are in the form of **tensors**. What are tensors? Tensors are PyTorch specific data structures that can efficiently handle data. They are basically n-dimensonal matrix-like structure. For example, a one dimensional data structure is simply a list. A 2 dimensional structure is a 2-D matrix. A 3D structure is a 3-D Matrix (having height, width and depth), and so on. So you can have very complex data types within tensors. Tensors make operations very efficient!\r\n",
        "\r\n",
        "We also added a `.requires_grad_()` in front of the random initialization of these parameters. This function is available only for tensors, so you cant randomly  append this function to any other datatype. *This is the pytorch function which tells PyTorch that these parameters need to be updated.*\r\n",
        "\r\n",
        "So, naturally, it needs gradients, in order to be updated (refer back to the parameter update formula). And once we tell PyTorch that these variables need gradients, it will *keep track of all operations you do on those variables. So it knows how to calculate the gradient for this. For example, if your model is a quadratic function, PyTorch will keep track of this, and hence know internally, how to calculate the gradient of the quadratic function. We'll be discussing this in more depth later*.\r\n",
        "\r\n",
        "Let us now define our model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NvK9LF6wtlz"
      },
      "source": [
        "def model(x,params=params): \r\n",
        "    w, b= params\r\n",
        "    return w*x +b "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCXH2FcfNkN3"
      },
      "source": [
        "Here comes another aspect of PyTorch. It expects all operations to be done between tensors only. So let us convert our data to tensors!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdWLlyNVNj09"
      },
      "source": [
        "x_tensor,y_tensor = torch.tensor(x).float() , torch.tensor(y).float() #PyTorch expects everything to be floats. You dont have to know this fact at this point\r\n",
        "x_tensor.shape, y_tensor.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NbPG9_FHO3fm"
      },
      "source": [
        "#Let us see if the model works on our data tensors...\r\n",
        "preds=model(x_tensor)\r\n",
        "preds.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cl24tr9EGjdj"
      },
      "source": [
        "Let us run this model on our inputs:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUkAj_U1G_v5"
      },
      "source": [
        "And let us finally calculate the loss function. Let us define this function. We use a predefined Mean Square Error Loss function, which is defined in PyTorch. If you ever want to find out how to import a functionality from a library - a quick Google search always helps. For example, you can type \"PyTorch MSE loss\", and you would surely get some results. Even if a functionality doesn't exist in the library, you may find discussions on some forum related to your query.\r\n",
        "\r\n",
        "By searching, we find that MSE is defined at a [specific submodule](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html) of the torch library - called the \"nn\". "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0HR_cyBG9tV"
      },
      "source": [
        "loss_fn=torch.nn.MSELoss() #Pytorch already implements the Mean Square Error Loss function for us. \r\n",
        "#It is equivalent to mean((pred-target)**2)\r\n",
        "\r\n",
        "def loss_LinearRegression(pred,target):\r\n",
        "    assert len(pred)==len(target) #we need to make sure that there are as many predictions as targets. If not, then there is some error in our code\r\n",
        "    return loss_fn(pred,target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Axo_Saj3HgEO"
      },
      "source": [
        "#let us test this function on our predictions and target values\r\n",
        "loss=loss_LinearRegression(preds,y_tensor)\r\n",
        "loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zZtobGAMjf6"
      },
      "source": [
        "Now, the next step is to calculate the gradient of our parameters with respect to the loss function. Remember, moving in the direction opoosite to the sign of the gradient would minimize the function of which the gradients are calculated (in this case, the loss function). **Calculating Gradients is as simple as typing the `.backward()` function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2v8E15QAS-pO"
      },
      "source": [
        "loss.backward()\r\n",
        "#gradients have now been calculated. Let us check these gradients"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RgUQZMjSHtPo"
      },
      "source": [
        "params.grad #the .grad method of tensors tell us the gradients of the loss function with respect to these parameters. \r\n",
        "#if you calculate the gradients by hand, you would get this value only!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4BpXrXMTMam"
      },
      "source": [
        "Finally, let us update our parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqB8xelfNY6j"
      },
      "source": [
        "lr=1e-4\r\n",
        "params.data-=lr*params.grad\r\n",
        "params #go back up and verify that the value of params have indeed changed!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UE7TQd4SW8fR"
      },
      "source": [
        "Let us also define a function that does one complete cycle."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZHz0ZGDb4Nv"
      },
      "source": [
        "params = torch.zeros(2).requires_grad_() #re-initializing our parameters\r\n",
        "\r\n",
        "def train_one_iteration(params,lr=1e-4): #In python 1e-4 is the same as saying 10^-4. Similarly, 2e5 would mean 2x(10^+5). And so on...\r\n",
        "  preds = model(x_tensor, params)\r\n",
        "  loss = loss_LinearRegression(preds, y_tensor)\r\n",
        "  loss.backward()\r\n",
        "  params.data -= lr * params.grad.data\r\n",
        "  params.grad = None #The details of this peice of code will be discussed in the Neural Networks section. But note: it is an essential step"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnSluEmj1-eo"
      },
      "source": [
        "Now let us finally run the Gradient Descent Algorithm. Remember, this is an iterative algorithm, and needs to be run multiple times. In each run, or iteration, the loss will reduce, and the predictions will hence improve! Let us first see how the model predicts initially, before training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2AwAjyX2iV2"
      },
      "source": [
        "#since we plan to visulaize the model in terms of a plot again and again, it is useful to write a function. \r\n",
        "#Remember, functions are an efficient way to avoid writing the same peices of code again and again\r\n",
        "def plot_linreg_model(params):\r\n",
        "    plt.plot(x_tensor,y_tensor)\r\n",
        "    pred=model(x_tensor,params).detach()\r\n",
        "    plt.plot(x_tensor,pred)\r\n",
        "    print('MSE Loss:',loss_LinearRegression(pred,y_tensor).item())\r\n",
        "\r\n",
        "plot_linreg_model(params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFcUSMeak7zB"
      },
      "source": [
        "for _ in range(50): train_one_iteration(params,lr=2e-7) #run this algorithm 50 times"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oarWvS933Hoz"
      },
      "source": [
        "plot_linreg_model(params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eS-5D7-X3hWh"
      },
      "source": [
        "The model has definitely improved! You can even see, the loss has improved. But if you go back to the section where we calculated the best fit line through the direct formula, you'd see that this fit is still off. So let us run it for some more iterations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSOi6Cml4B60"
      },
      "source": [
        "for _ in range(1000): train_one_iteration(params,lr=2e-7)\r\n",
        "plot_linreg_model(params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tv6-M4-z4Xi2"
      },
      "source": [
        "At this point, you'd see that the improvement is almost very small. This is because we are close to the minima already. When we're close to the minima, the gradient becomes very small (you can visualize this by taking the example of the quadratic function example we introduced earlier). So the parameter update too is very small. So now that we know the model is not improving anymore, let us leave it at that. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXAGlqHx5G9x"
      },
      "source": [
        "#### Inference\r\n",
        "Now, let us see what this model predicts the stock price to be at the end of 2018, and compare it with the actual closing price of the Google stock at the end of 2018. Remember, our input `x` is the number of days passed since the reference date. So we use a function which we already defined, to find out the number of days between 31 Dec 2018, and the reference date."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5YZ_Yz1O411S"
      },
      "source": [
        "day=datetime(2018,12,31)\r\n",
        "days_elapsed= calculate_days_since(day)\r\n",
        "days_elapsed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zeDlhSMQlcBk"
      },
      "source": [
        "model(days_elapsed,params).item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ju0O803k7pqj"
      },
      "source": [
        "Let us look at the actual price at the end of 2018.\r\n",
        "\r\n",
        "![image](https://drive.google.com/uc?id=1Kkbbqvk33TeqYHxmXoeWJW1mh5rgnFAM)\r\n",
        "\r\n",
        "You can see, that at the end of 2018, the stock price was about USD 1037. This is not very far from the prediction our own model!\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-84i7cm15yIB"
      },
      "source": [
        "Congratulations, you have successfully built your Linear Regression Model!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GfceQNK_eZt"
      },
      "source": [
        "### Review:\r\n",
        "Below we've given an exercise for you to try out. But before that, there are a few tips and review points, that you should go through, think over, and make sure you understand each of them. These points are the most important points of this entire session - and not surprisingly, these are the concepts and ideas that will actually help you be a good Machine Learning practitioner.\r\n",
        "\r\n",
        "#### **Some Tips:**\r\n",
        "1. Machine Learning is rapidly evolving to be more about efficiency, rather than depth of knowledge. Just having the skill to implement a model is far more valuable than the vast theory of someone who spent years reading books. Knowing your concepts is more important than the unnecessary details. For example, we didn't have to know how to calculate gradients in order to do gradient descent. If you ask someone who has learnt Machine Learning the hard way, they would, almost anyday, say that it is not possible to do Gradient Descent without knowing enough calculus to calculate gradients. But clearly, that is not true anymore. This is the magic of the wonderful frameworks that have been developed by the AI community to facilitate learning. But, that does not mean you should skip the concepts comepletely. Not having to do something, and not knowing something at all are two completely different things. You cannot escape learning what gradients are, and how they are calculated. Just that, you don't Need to calculate them. Its like, having a calculator to do your math is a luxury, but a person who does not know how to add numbers at all cannot thrive. \r\n",
        "\r\n",
        "2. Having said that, this is the approach you should follow. You should think smart - all functionalities need not be written from scratch. Often, a pre-existing code might be the way to go, and maybe even more efficient. Ofcourse, this does not apply in every context (for example, you cannot simply use someone's copy-righted code for commercial purposes, because of legal implecations), but wherever it applies, think in that direction. \r\n",
        "\r\n",
        "3. Finally, the ability to search for your queries is The MOST valuable skill you will learn as a developer. Knowing what and how to Google search your queries is a skill, which comes over time, but is a wonderful skill to acquire. You need not memorize syntaxes of commands, or even know the complete functionality a function may have. You can always do a quick search over documentations and forums. But again, Do Not think of this as an excuse to not learn at all. Just till it doesnt not come to you naturally, you may use these tools as an aid. \r\n",
        "\r\n",
        "\r\n",
        "#### **Review Questions**\r\n",
        "These are non-evaluative, but highly recommended to go through. Make sure you clearly know the answer to each of these concepts. The answers to all these questions are somewhere in this notebook, so if you find yourself unclear with a concept, go back up and find the answer!\r\n",
        "\r\n",
        "1. What is a Machine Learning model in terms of code? Can you write the syntax of a model in a simple pseudo-code form?\r\n",
        "2. We imported many important libraries in this notebook. Whenever a library was imported, the reason and the goal was mentioned. Can you enlist all the libraries used in this notebook, and why they were used?\r\n",
        "\r\n",
        "3. Can you describe in 3-5 one-line statements, the steps to carry out Linear Regression?\r\n",
        "\r\n",
        "4. what is a tensor?\r\n",
        "\r\n",
        "5. Why are dataframes so popular with handling tabular data?\r\n",
        "\r\n",
        "6. What is a loss function? Which loss function did we use for Linear Regression?\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Ol3fmKg_Vwl"
      },
      "source": [
        "# Exercise (Evaluative):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ML6Ipq3pJ6rP"
      },
      "source": [
        "## Problem 1: Building your own Linear Regression Stock Prediction Model.\r\n",
        "\r\n",
        "Can you carry out Linear Regression using Gradient Descent for 2 other stocks. Amazon (AMZN), and NASDAQ (NDAQ). You need to redefine all functions, and may use the functionalities that we have already defined. In addition to the number of the days passed since the reference date, also use the volume of trade as an input feature. (Hence, you now have 3 parameters, w1,w2 and b). \r\n",
        "Report the parameters and loss in terms of a dataframe. Also plot the linear regression model with the datapoints (as shown above) for both these stocks (stock price vs number of days passed). All the major steps will remain the same. You may need to do some minor tweaks to consider the extra input feature."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yC_rmUGiQT0w"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkGy9oXnQU-8"
      },
      "source": [
        "# Problem 2: Non Linear Regression\r\n",
        "We saw Linear Regression, which aimed at fitting a straight line into the dataset. But what if we wish to fit a non-linear curve into the data. This can be done by replacing a linear function with a polynomial function. \r\n",
        "\r\n",
        "Instead of:\r\n",
        "```\r\n",
        "y= w.x + b\r\n",
        "```\r\n",
        "We could use a non-linear function:\r\n",
        "```\r\n",
        "y=w1.(x**3) + w2.(x**2) + w3.(x) + b\r\n",
        "```\r\n",
        "\r\n",
        "So now, we have a cubic function, instead of a linear function. This can be used to fit a non-linear curve. Generally speaking this is more flexible than a linear curve. It can act as a cubic function, a quadratic function (if you set w1 as 0), a linear function (if w1 and w2 both are 0), or something in between these curves. The method of regression remains the same. Can you implement Non Linear Regression using the cubic function as the model?\r\n",
        "\r\n",
        "Additional tips:\r\n",
        "Cubing a value exponentially increases it in magnitude. Computer CPUs cannot handle very large values. This is known as Model Explosion. So we first divide each input by 100, and then apply this model. \r\n",
        "\r\n",
        "Within your model function: \r\n",
        "Do the following transformation on the input `x`\r\n",
        "\r\n",
        "`input=torch.div(x.detach(),100)`\r\n",
        "\r\n",
        "Then use the input variable as the model input. \r\n",
        "\r\n",
        "`w1*(input**3) + w2*(input**2) + w3*(input) +b` .\r\n",
        "\r\n",
        "---\r\n",
        "\r\n",
        "Use 20 iterations, and a learning rate of 2e-8.\r\n",
        "\r\n",
        "Does this model perform better than the previous linear regression model? \r\n",
        "\r\n",
        "Additionally, try different Learning Rates to see if you can improve the results.\r\n",
        "Show 5 different reasonable learning rates, and show the plots."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbwyzD_qSsZZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBIvgo-PTIRK"
      },
      "source": [
        "plot_linreg_model(params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-13QW48TRhZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}