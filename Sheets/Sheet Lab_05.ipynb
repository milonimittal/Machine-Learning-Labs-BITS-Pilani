{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "05 Clustering Algorithms in Machine Learning.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPK74JRy39bP"
      },
      "source": [
        "#05 Clustering Algorithms in Machine Learning \r\n",
        "###  K Means and K Nearest Neighbors Algorithms\r\n",
        "\r\n",
        "Welcome to the fifth session of the Practical Machine Learning sessions, for the Machine Learning course. In this lab session, we will move slightly towards a relatively overlooked domain of Machine Learning - *Unsupervised Learning*. Till Now, and for the most of the part in the future, we will be looking at *Supervised Learning* problems. So, first of all, what is the difference between supervised learning and unsupervised learning?\r\n",
        "\r\n",
        "If I were to tell you the textbook definition of the difference between the two,I would simply tell you, that unsupervised learning requires labels, and supervised learning does require labels. Yes, that is a difference, but in introspection, it is really not a good way to tell the difference between the two. We simply answered the question, \"What do these learning methods *require* in order to learn\", but not the most important question - *What on earth IS unsupervised learning, or supervised learning, and what is the difference in terms of the approach?*\r\n",
        "\r\n",
        "So, lets spend some time to understand that first. \r\n",
        "All of the supervised training algorithms we saw till now had a very common theme. We had some data. We build a model which takes in inputs, and gives and output (the input and output may not necessarily be different things. For example, in the text generator example, the input itself was the target).\r\n",
        "\r\n",
        "And we try to *optimize* the model such that the prediction outputs and target variables are as close to each other as possible. By this, we intend to build a model that knows how to map those inputs to the real world targets. For example, if you were to build a model that can generate music for you, you provide your model with lots of examples of music peices, and expect the model to learn how to make music, *without explicitly teaching it music theory, or the idea of melody, etc*, and only through example. The idea is - we try to optimize the model with a specific goal in mind that we wish the model performs.\r\n",
        "\r\n",
        "This is not the case for unsupervised learning. We dont build models with a specific goal in mind. The objective is to simply identify whatever patterns are observable in the data. Like, What is the mathematical distribution? How dense is the data? What is the range of the values in the data? How many clusters are observable? Is there a certain visible relation between different types of data?(like, if one increases, does the other increase too?) and so on.\r\n",
        "\r\n",
        "Unsupervised learning is used to identify patterns, whereas supervised learning is used to get specific results from the data. This is the correct method to differentiate between these two!\r\n",
        "\r\n",
        "---\r\n",
        "\r\n",
        "Now, coming to our topic today - *Clustering Algorithms!* We'll be looking at two algorithms -  **K Means** and **K Nearest Neighbors** Algorithms, and we'll explore both separately! So let's get started..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJM48MJFYlNL"
      },
      "source": [
        "### K Means Algorithm\r\n",
        "\r\n",
        "Take a look at this data below.\r\n",
        "\r\n",
        "<figure><center>\r\n",
        "<img src='https://drive.google.com/uc?id=1gFNxPtQhVb8ekxZN4XV6NWZhGHgSwpFy' width='50%'>\r\n",
        "<figcaption>The Social Network</figcaption>\r\n",
        "\r\n",
        "</center></figure>\r\n",
        "\r\n",
        "This is a social network graph. It is basically a visualization of who is a connection of whom. For example, if this network belonged to Facebook, the image would depict which people are friends on facebook, and who aren't. People who are friends on facebook have a faint grey line joining their profile images. You might notice that there are some distinct groups in this photo, which appear as blobs. You can see two major such blobs, one on the left and one on the right. These are called *clusters* of data. \r\n",
        "\r\n",
        "Lets say that people in the left blob are college going kids, and on the right are business professionals. Obviously, college going kids are more likely to know each other than they would know business professionals, and similarly business professionals are more likely to know each other, rather than college going kids, only because of the shared experiences and backgrounds. Hence, you would expect college kids to be friends with each other, far more than you would expect college kids being Facebook friends with business professionals. Hence these two clusters. \r\n",
        "\r\n",
        "Within each cluster, people are connected to each other via Facebook. Obviously it is not true, that no college kids are connected to any business professionals, and vice versa. But still, there is such a stark lack of such connections, that the graph almost seems like its divided into two major clusters. \r\n",
        "\r\n",
        "Just by looking at the data, you can tell that there are 2 clusters in the data. You don't even have to know which person goes to college, and which is a business professional. We would like our machines to do that for us - without necessarily having to tell which person belongs to which category. Hence the concept of Unsupervised Learning!\r\n",
        "\r\n",
        "Why would we want to identify clusters? Let's take the example of Facebook itself. Facebook can't recommend the same Ads to everyone. It obviously needs to optimize this. But Facebook knows that if some college goers are interested in a product, say Fidget Spinners (even though that's not a thing anymore!) , others will be too! And they will be much more likely to buy these, than business professionals. So Facebook would recommend Fidget Spinners to college goers rather than business professionals. On the other hand, if Facebook realizes that there is a trend of buying international holiday trip packages, it would rather recommend those to all business professionals, rather than college kids, which are much less likely to afford one! Ofcourse to do all this, Facebook might  not necessarily know the fact that which group is which in profession.\r\n",
        "\r\n",
        "Hope this idea is clear!\r\n",
        "\r\n",
        "In Machine Learning, a great algorithm to carry out unsupervised clustering is the **K Means** Algorithm.\r\n",
        "\r\n",
        "The objective of the K Means Algorithm is to create K different clusters, that are visible separated from each other. The procedure is as follows:\r\n",
        "\r\n",
        "1. create K random data points, called as *centroids*. Each of these centroids will form a separate cluster later on.\r\n",
        "2. For each datapoint, find out which centroid is the datapoint closest to. Whichever centroid it is closest too, assign the datapoint the same group as the centroid. So at the end, each datapoint will be assigned a group, containing exacly one centroid.\r\n",
        "3. Update the value of centroids as the Mean of all the datapoints that exist in the group.\r\n",
        "4. Repeat Steps 2 and 3 until you get desired results. During step 2, the group assigned to a datapoint may change, depending on which centroid it is closest too at this point (after updating the position of centroids).\r\n",
        "\r\n",
        "Imagine what happens during step3. By updating the value of the centroid as the mean of all the datapoints, we are essentially *pulling* the centroid to where there is more density of data. If this process is repeated multiple times, we will eventually pull the centroid to a position where this high density of data, or in other words, a blob/cluster exists. Here is a quick visualization of the K Means algorithm.\r\n",
        "\r\n",
        "<figure><center>\r\n",
        "<img src='https://drive.google.com/uc?id=15T7mCRh1AsTwFgFRxvrdXDkHRPcqiREZ' width='50%'>\r\n",
        "</center></figure>\r\n",
        "\r\n",
        "In today's session we'll be looking at a very interesting problem - We will be identifying the crime hotspots of a city - meaning, identifying which areas are more prone to crimes and which are safer.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGUn_BRNFKgF"
      },
      "source": [
        "#### Case Study: Crime Hotpots in Vancouver\r\n",
        "\r\n",
        "We have some data about crimes in the city of Vancouver, Canada. The data includes their nature, the time of occurence, the location, etc. Our job is to identify the areas of the city where the crime is more likely to occur. This can help the local Police Department deploy enough personnel, so that they are ready, equipped, and well numbered to tackle crimes. \r\n",
        "\r\n",
        "This can also help people like us, who would want to find new homes. We would obviously like to buy a house in a safer locality, rather than a crime-prone locality.\r\n",
        "\r\n",
        "So let us first download the [dataset](https://www.kaggle.com/wosaku/crime-in-vancouver) from kaggle. Before that, you need to upload the *kaggle.json* file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ti0pCMBnRFxc"
      },
      "source": [
        "#run only once per session\r\n",
        "%cd \r\n",
        "from google.colab import files\r\n",
        "uploaded = files.upload()\r\n",
        "!mkdir -p ~/.kaggle/ && mv kaggle.json ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json\r\n",
        "!kaggle datasets download -d wosaku/crime-in-vancouver\r\n",
        "!mkdir -p vancouver_crime && unzip crime-in-vancouver.zip -d vancouver_crime\r\n",
        "%cd vancouver_crime\r\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnTvAc0TGWXC"
      },
      "source": [
        "As you can see, there is a file called the \"crime.csv\" in this dataset. So to read csv files, we'll be using the pandas library. We'll also be using the matplotlib library to visualize the locations of crimes in a graph plot. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYUSFfDcZdij"
      },
      "source": [
        "import pandas as pd\r\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CB3I6MAkb2y0"
      },
      "source": [
        "df=pd.read_csv(\"crime.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdmUQ8uOb4m4"
      },
      "source": [
        "print(len(df))\r\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1nzOXeCGse4"
      },
      "source": [
        "So there are 530,652 cases of crimes in this dataset. You can see, the dataset provides us with the coordinates of the crimes, as well as the Lattitudes and Longitudes and the times. They also provide us with the type of crimes. However, a crime is a crime, and we don't necessarily need to know the type of crime while looking for a house, or while the police department deploys personnel in different regions. But still, let us try to see what all crimes happen in Vancouver!\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtexFrBjHgO_"
      },
      "source": [
        "df.TYPE.unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXXEDnG5HkWg"
      },
      "source": [
        "Before we start working with this data, let us remove any empty records (for example, a policeman, while recording the crime, might not have filled the coordinates, or might not have thought of it as relevant to the crime). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phHMI0pVxWrM"
      },
      "source": [
        "df.dropna(inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZARxPw3HyDf"
      },
      "source": [
        "So let us now visualize this dataset. Let us plot on a graph, all the X cordinates and the Y cordinates of the crimes, so we get an idea of where do crimes take place, and how many crimes take place in any locality (a rough idea!). But before that, lets see the actual map of Vancouver!\r\n",
        "\r\n",
        "<figure>\r\n",
        "<img src='https://drive.google.com/uc?id=1lEOKrWqTal2n9ZGSll1g8A7K4n6gICpI' width='40%'>\r\n",
        "</figure>\r\n",
        "\r\n",
        "This map also shows the Neighborhoods in Vancouver, which is provided to us in the dataset as well. \r\n",
        "\r\n",
        "Let us now plot all crimes on a graph, and see where in Vancouver are crimes found!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWpC1ngYw5ke"
      },
      "source": [
        "plt.scatter(df.X,df.Y,s=0.001) #s is used to control the size of datapoints on the graphs. There are so many datapoints, that if you use the normal size, all the datapoints will get merged. Hence, use a small 's'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9Gnhp42L-tX"
      },
      "source": [
        "You can see, that there are crimes all over the city. However, you may observe that in some parts, the density of crimes is higher than the rest of the city (For example, in the northern parts of the city)! \r\n",
        "\r\n",
        "So let us see, which neighborhoods have more crime than others. Below, we plot a colour coded map showing different neighborhoods, as well as a bar graph, showing the number of crimes per neighborhood."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GvH09-wAs1xf"
      },
      "source": [
        "category='NEIGHBOURHOOD'\r\n",
        "for name,group in df.groupby(category):\r\n",
        "    plt.scatter(group.X,group.Y,label=name)\r\n",
        "plt.legend(bbox_to_anchor=(1, 1), loc='upper left');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOJdE_KWsRMV"
      },
      "source": [
        "count={i:len(df[df[category]==i]) for i in df[category].unique()} # a dictionary that can help us keep track of which neighborhood has how many crimes\r\n",
        "for i in count:\r\n",
        "    plt.bar(i,count[i])\r\n",
        "plt.xticks(rotation='vertical');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOvcXh-HM3Zt"
      },
      "source": [
        "Now that we know the number of cases per neighborhood, Let us jump to our ultimate objective - to differentiate neighborhoods by the number of crimes. We would like to create *3 clusters* - High crime rate neighborhood, Moderate crime rate neighborhood, and Low crime neighborhood. Let us first also plot the number of crimes in a straight line."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KngMhyiOsROz"
      },
      "source": [
        "for i in count: plt.scatter(count[i],0,label=f'{i}: {count[i]} cases')\r\n",
        "plt.xlabel('Number of Crimes')\r\n",
        "plt.legend(bbox_to_anchor=(1, 1), loc='upper left')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnmg--OYNeuA"
      },
      "source": [
        "You'll notice some things - The Central Business District has more crimes than any Neighborhood, and by a HUGE margin. Obviously that will be clustered as a high crime region. Then come low crime regions and moderate crime regions. We would like our model to automatically detect which district is lower/moderate in crime (based on blobs).\r\n",
        "\r\n",
        "Let us begin by initializing centroids. To do this, we'll be using the NumPy library, which is a numerical processing library in python. It provides us with mathematical tools, which are otherwise computationally and memory wise - intensive. NumPy also provides us with *arrays* which is a concept similar to tensors in PyTorch. However, NumPy arrays are 1-Dimensional only. We'll be using these arrays to do calculations on our data. \r\n",
        "\r\n",
        "Why numpy arrays and not lists? This is because NumPy arrays and functions are optimized to run at C/C++ speed, which is many orders of magnitude faster than Python itself. This is suitable for large amounts of data, and here we're dealing with hundreds of thousands of datapoints. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqvwQXkJsRQh"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2tyvpgfCgXs-"
      },
      "source": [
        "values=list(count.values())\r\n",
        "clusters=['Low','Moderate','High']\r\n",
        "colour_scheme={'Low':'green',\r\n",
        "               'Moderate': 'orange',\r\n",
        "               'High': 'red'}\r\n",
        "num_clusters=len(clusters)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5HLptfYDbdR"
      },
      "source": [
        "#INITIALIZE CENTROIDS\r\n",
        "centroids=np.sort(np.random.uniform(low=min(values),high=max(values),size=num_clusters))\r\n",
        "centroids #initial centroids. Represent the number of crimes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5TiZ-lnO7qN"
      },
      "source": [
        "As we mentioned before, K Means intrinsically doesn't know which cluster belongs to what kind of crime rate. But for better understanding, we will assign the categories of crime rates (Low, moderate and high). So to do that, what we've done is we've made sure that the randomly generated centroids are always in ascending order. These represent ascending order of crime rates. Correspondingly, the labels will automatically become Low Crime Rate, Moderate Crime Rate and High Crime Rate. \r\n",
        "\r\n",
        "We've also assigned colour codes to these - green being low crime regions, orange for moderate and red for high crime regions. \r\n",
        "\r\n",
        "Below, we've plotted the crime rates, along with the centroids, marked with crosses. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8Sb6zXIze9y"
      },
      "source": [
        "plt.scatter(list(count.values()),[0]*len(count))\r\n",
        "plt.scatter(centroids,[0]*num_clusters,marker='X',s=100)\r\n",
        "plt.xlabel('Number of Crimes');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJZnyyoGP9SO"
      },
      "source": [
        "We'll make lists corresponding to each of these categories. For example, if a neighborhood's crime frequency is closest to the centroid corresponding to Low Crime rates, we will add the neighborhood to the list corresponding to low crime rates"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOjTq_rUsRSk"
      },
      "source": [
        "categories=[[] for _ in range(num_clusters)]\r\n",
        "for i in count: categories[np.argmin([abs(count[i]-o) for o in centroids])].append(i)\r\n",
        "categories"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZohxXydLQWE3"
      },
      "source": [
        "You can see, there are three lists in the variable `categories`, containing neighborhoods that lie closest to the corresponding centroids (randomly initialized, so the may not make proper sense right now).\r\n",
        "\r\n",
        "So now, We will assign the crime rate category to each datapoint that belongs to a certain category. This way, we can colour code the city, based on the location of the crime."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2qGJkKaD6UA"
      },
      "source": [
        "df['groups']=None\r\n",
        "for k,o in enumerate(categories):\r\n",
        "    for i in o: df.loc[df['NEIGHBOURHOOD']==i,'groups']=clusters[k]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IN82xac_JWEz"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDbsSmndRGyA"
      },
      "source": [
        "We've added a  column in the dataframe, called 'groups', which contains the crime rate category, as done by K means algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_wkLj74NaPF"
      },
      "source": [
        "category='groups'\r\n",
        "for name,group in df.groupby(category):\r\n",
        "    plt.scatter(group.X,group.Y,label=name,c=colour_scheme[name],s=0.1)\r\n",
        "plt.legend(bbox_to_anchor=(1, 1), loc='upper left');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvr8IrETRPk_"
      },
      "source": [
        "You can see, that each region is colour coded as per crime region. Note: you MIGHT be encountering a small problem - you might not be seeing all the categories. You might be seeing less than 3 categories. This is because K Means is *very sensitive to initialization*. Because the initialization is random, it may be possible that some centroids do not lie closest to any datapoints. Each datapoint might be closest to any centroid But a particular centroid. If you're encountering such a problem, go back up, and reinitialize the centroids, and run all the cells of code upto the one just above this peice of text, and repeat this until all three categories are not visible. (Even if you are not encountering this issue, its a fun exercise to see this happen!).\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tNYNxapSsFF"
      },
      "source": [
        "Next Step? Update the centroids as the mean of all datapoints that have been assigned to that cluster."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jN4EIswO9TxR"
      },
      "source": [
        "for k,o in enumerate(categories):\r\n",
        "    if o: centroids[k]=np.mean([count[i] for i in o])\r\n",
        "centroids"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SyI9VHyLSzmh"
      },
      "source": [
        "Now all we have to do is repeat this procedure (excluding the random initialization - that needs to be done only once). So Let us write all the peices of code together, and see it work in one go!\r\n",
        "\r\n",
        "Try running the next two cells of code multiple times to see if you get slightly different results each time. This is because of the randomness during initialization. But nevertheless, you will get satisfactory results. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WIP0t9eeRe1f"
      },
      "source": [
        "#Step 1: initialize centroids\r\n",
        "centroids=np.sort(np.random.uniform(low=min(values),high=max(values),size=num_clusters))\r\n",
        "centroids #initial centroids"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNQS6rH2P9MS"
      },
      "source": [
        "for iteration in range(5): #We run this model for 5 iterations\r\n",
        "    #Step 2: categorize all points into one or the other groups\r\n",
        "    categories=[[] for _ in range(num_clusters)]\r\n",
        "    for i in count: categories[np.argmin([abs(count[i]-o) for o in centroids])].append(i)    \r\n",
        "    for k,o in enumerate(categories):\r\n",
        "        for i in o: df.loc[df['NEIGHBOURHOOD']==i,'groups']=clusters[k]\r\n",
        "    #Step 3: update centroids\r\n",
        "    for k,o in enumerate(categories):\r\n",
        "        if o: centroids[k]=np.mean([count[i] for i in o]) \r\n",
        "\r\n",
        "    # Visualize\r\n",
        "    for name,group in df.groupby(category):\r\n",
        "        plt.scatter(group.X,group.Y,label=name,c=colour_scheme[name],s=0.1);\r\n",
        "    plt.title(f'iteration: {iteration+1}')\r\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYlNtG6kTYde"
      },
      "source": [
        "And there you have it! You have successfully built your own Crime Hotspot Detection system! And using 4 simple steps only - Initialize, Assign, Update, Repeat! Congrats on that!\r\n",
        "\r\n",
        "But there is something we can, and should do. These 4 steps should not so many lines of code. We should *refactor* the code into functions, so that the model becomes easier to run and understand. So we've build a basic structure to help us with all the major steps. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mncbn6xNh4lL"
      },
      "source": [
        "def initialize_centroids(values,num_clusters):\r\n",
        "    centroids=np.sort(np.random.uniform(low=min(values),high=max(values),size=num_clusters))\r\n",
        "    return centroids\r\n",
        "\r\n",
        "def count_frequencies_in_df(df,group_by='NEIGHBOURHOOD'):\r\n",
        "    df.dropna(inplace=True)\r\n",
        "    df.count_freqs={i:len(df[df[group_by]==i]) for i in df[group_by].unique()}\r\n",
        "\r\n",
        "def categorize(df,centroids,clusters,group_by='NEIGHBOURHOOD'):\r\n",
        "    num_clusters=len(clusters)\r\n",
        "    categories=[[] for _ in range(num_clusters)]\r\n",
        "    # for i in df.count_freqs: categories[np.argmin([abs(df.count_freqs[i]-o) for o in centroids])].append(i)  \r\n",
        "    for i in df.count_freqs:\r\n",
        "        argmin=np.argmin([abs(df.count_freqs[i]-o) for o in centroids])\r\n",
        "        categories[argmin].append(i)\r\n",
        "    \r\n",
        "    df['groups']=None  \r\n",
        "    for k,o in enumerate(categories):\r\n",
        "        for i in o: df.loc[df[group_by]==i,'groups']=clusters[k]\r\n",
        "    return categories\r\n",
        "\r\n",
        "def update_centroids(centroids, categories):\r\n",
        "    for k,o in enumerate(categories):\r\n",
        "        if o: centroids[k]=np.mean([df.count_freqs[i] for i in o])\r\n",
        "    return centroids\r\n",
        "\r\n",
        "def visualize(df, category_column, iteration ):\r\n",
        "    for name,group in df.groupby(category_column):\r\n",
        "        plt.scatter(group.X,group.Y,label=name,c=colour_scheme[name],s=0.001);\r\n",
        "    plt.title(f'iteration: {iteration+1}')\r\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFT2kZdzT9wo"
      },
      "source": [
        "And let us see an entire Crime Hotspot detection model running in only 7 lines of code! \r\n",
        "\r\n",
        "PS: Try running the cell below multiple times, to get slightly different results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGEFDGzYjVpW"
      },
      "source": [
        "#Entire K means in just 7 lines of code , excluding comments, ofcourse\r\n",
        "\r\n",
        "#step0\r\n",
        "group_by='NEIGHBOURHOOD'\r\n",
        "count_frequencies_in_df(df,group_by=group_by) #method which creates a data structure based on which distinction is to be done\r\n",
        "# step1\r\n",
        "centroids=initialize_centroids(values, num_clusters)\r\n",
        "for i in range(5):\r\n",
        "    #step2\r\n",
        "    categories = categorize(df,centroids,clusters,group_by=group_by)\r\n",
        "    # step3\r\n",
        "    centroids = update_centroids(centroids,categories)\r\n",
        "    visualize(df,'groups',iteration=i)\r\n",
        "    #repeat step 2 through 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "la0OFoSFUR9g"
      },
      "source": [
        "Just for comparison, here is the map of all crimes. Try noticing if high density regions have been correctly marked as red, and if low density regions correcly as green? What about orange?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_lIH-vJpIH9o"
      },
      "source": [
        "plt.scatter(df.X,df.Y,s=0.001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjH9sJ-5fCCS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRazcvSk-o39"
      },
      "source": [
        "Now, let us move onto the second algorithm that we'll look at today - the **K Nearest Neighbors** algorithm.\r\n",
        "\r\n",
        "### K Nearest Neighbors\r\n",
        "K nearest Neighbors is a very simple algorithm. You are given some data. Now, suppose when you get some new data point, and you wish to either classify or predict a value, you simply look at its neighbors. \r\n",
        "\r\n",
        "1. If you want to classify a datapoint into a category, look at K nearest datapoints. Whichever category occurs the most, is most likely to be the class of the datapoint as well.\r\n",
        "\r\n",
        "2. If you want to conduct regression, look at the values of K nearest datapoints. You can take the average of the taget values of these datapoints, and it will be a likely estimate of the target value of your datapoint as well.\r\n",
        "\r\n",
        "The idea is simply as follows - \"*Birds of a feather flock together...*\".\r\n",
        "\r\n",
        "This is technically a supervised learning problem, but still is closer to unsupervised learning than other supervised learning algorithms. There is no learning of parameters, only observing the position of other datapoints relative to the input data point."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SvaSfQne4Xe"
      },
      "source": [
        "So previously we tried to cluster neighborhoods based on the number of crimes per neighborhood. Now we will try to classify each locality with respect to the most common crime in that area. We will analyse K nearest Neighbors and figure out which crime is the most common. Ofcourse, that is the most likely crime to happen at that particular location. \r\n",
        "\r\n",
        "Let us begin with some random coordinates and visualize it on the map!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WyeLrGyG-g3w"
      },
      "source": [
        "coordinates=(494000,5453000)\r\n",
        "plt.scatter(df.X,df.Y,s=0.001)\r\n",
        "plt.scatter(*coordinates,c='RED',marker='X',s=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7LdLzDnQQTn"
      },
      "source": [
        "x,y=coordinates"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgrXPSIVf1bl"
      },
      "source": [
        "The next step is to calculate the *Distance* of this coordinate from other datapoints. This will be used to find the the nearest neighbors! But we won't write a for loop - that is awfully slow. Infact, we will use function that NumPy provides, which run at C/C++ speed. For 2D cordinates, the distance is defined as the euclidean distance between the two, given by - \r\n",
        "\r\n",
        "$distance((x_1,y_1), (x_2,y_2)) = \\sqrt{(x_1 - x_2)^2 + (y_1 -y_2)^2}$\r\n",
        "\r\n",
        "We write all these distances in a new column of the dataframe - the 'Distance column'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9HvWkhaQ8wO"
      },
      "source": [
        "df['Distance']=np.sqrt(np.square(x-df.X) + np.square(y-df.Y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bawfukxCgSOF"
      },
      "source": [
        "Now, we need to find nearest neighbors. To do that, we will simply sort the dataframe with respect to the distances, and simply choose the top K rows. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OuZjnnfrVFte"
      },
      "source": [
        "df.sort_values('Distance',inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMRqAaMWhA79"
      },
      "source": [
        "You can see that the rows are arranged in the ascending order of the distances."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SS9A5BAaXN0M"
      },
      "source": [
        "df.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-ydPufphF2f"
      },
      "source": [
        "Choose a K. There is no definite method to choose K. But for this case, we'll choose K as 1000. Meaning, we'll be analysing the most probable crime on a given pair of coordinates, based on 1000 surrounding points. Whichever crime occurs in majority in these 1000 points, is the most likely crime in that location as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1_yo7jxWFUB"
      },
      "source": [
        "k=1000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enSpWJ-iiHbE"
      },
      "source": [
        "Finally, let us carry out the prediction, which is simply the crime which occurs most frequently in the K points."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZbntWnyiG2v"
      },
      "source": [
        "prediction = df[:k].TYPE.value_counts().index[0]\r\n",
        "prediction"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QCDsy2ZhzTo"
      },
      "source": [
        "Hence, we have found the most probable crime on those coordinates. Let us also see, that, out of 1000 datapoints, how many of each crimes occur, and verify that the most frequent crimes is the same as our prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xh5xrJRTl4Fx"
      },
      "source": [
        "df[:k].TYPE.value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSPoPj8piXym"
      },
      "source": [
        "Let us visualize this! We plot only these K points, and see, the number of crimes in the area, and visually verify that most common crime in the area is the same as our prediction. For better visual aid, we've enlarged the most probable crime with respect to other crimes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFT0YYM9YkX8"
      },
      "source": [
        "for name,group in df[:k].groupby('TYPE'):\r\n",
        "    plt.scatter(group.X,group.Y,label=name,s=50 if name is prediction else 1 )\r\n",
        "plt.scatter(x,y,c='RED',marker='X',s=200,label='Target')\r\n",
        "plt.legend(bbox_to_anchor=(1, 1), loc='upper left')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaVtHu7hi2wg"
      },
      "source": [
        "And thats it! You've built your own crime predictor as well! With such less effort, that too!\r\n",
        "\r\n",
        "But again, we've written way too much of code for such simple tasks. We should refactorize the code. Let us write some appropriate functions, carrying out the same tasks. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HfJcUyVZwsZ"
      },
      "source": [
        "def visualize_target(coordinates:tuple, df):\r\n",
        "    x,y=coordinates\r\n",
        "    assert df.X.min()<=x<=df.X.max(),f'x cordinate should lie in Vancouver City - between cordinates {df.X.min()} and {df.X.max()}.'\r\n",
        "    assert df.Y.min()<=y<=df.Y.max(),f'y cordinate should lie in Vancouver City - between cordinates {df.Y.min()} and {df.Y.max()}.'\r\n",
        "    plt.scatter(df.X,df.Y,s=0.001)\r\n",
        "    plt.scatter(x,y,c='RED',marker='X',s=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5v2qd9mivxj"
      },
      "source": [
        "def euclidean_2d_distance(x,y,df):\r\n",
        "    return np.sqrt(np.square(x-df.X) + np.square(y-df.Y))\r\n",
        "\r\n",
        "def get_k_nearest_neighbors(df,target_coordinates,k=1000,distance_func=euclidean_2d_distance):\r\n",
        "    df['Distance']=distance_func(*coordinates,df)\r\n",
        "    df.sort_values('Distance',inplace=True)\r\n",
        "    return df[:k]\r\n",
        "\r\n",
        "def get_prediction(df):\r\n",
        "    prediction = df.TYPE.value_counts().index[0]\r\n",
        "    return prediction\r\n",
        "\r\n",
        "def visualize_prediction(df,pred=None,k=None):\r\n",
        "    if pred is None: pred=get_prediction(df)\r\n",
        "    if k is None: k= len(df)\r\n",
        "    \r\n",
        "    for name,group in df[:k].groupby('TYPE'):\r\n",
        "        plt.scatter(group.X,group.Y,label=name,s=50 if name is prediction else 1 )\r\n",
        "    plt.scatter(x,y,c='RED',marker='X',s=200,label='Target')\r\n",
        "    plt.legend(bbox_to_anchor=(1, 1), loc='upper left')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "015Z4aKKfL9-"
      },
      "source": [
        "# KNN in 4 lines\r\n",
        "visualize_target(coordinates,df) #line1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uGTcfk-SoZf8"
      },
      "source": [
        "pred=get_k_nearest_neighbors(df,coordinates) #line2\r\n",
        "get_prediction(pred) #line3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5cfwS9y2ojOV"
      },
      "source": [
        "visualize_prediction(df,pred=pred,k=1000) #line4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rX4VfKn9ju8Z"
      },
      "source": [
        "Try running this algorithm with a different pair of coordinates!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DstMXXsj0Cf"
      },
      "source": [
        "## Review and Tips\r\n",
        "\r\n",
        "In this session, we studied about clustering algorithms in Machine Learning, which is a step towards Unsupervised learning in ML. In both the algorithms above, we did not direct our model towards a specific goal, but did general inference (patter identification) on the data. And still got some really meaningful results. That being said, one should not think that unsupervised leanring is not useful at all. It is very useful in its own applications. \r\n",
        "\r\n",
        "#### Tips\r\n",
        "The only useful tip that you need to learn from this session is - Refactoring code. Most Machine Learning practitioners do not pay attention to this, and this creates a problem for them later on. Why? because if you simply make a good structure of code, you need not rewrite the entire structure if you want your model to perform just slightly differently. As you will see in the exercises below (in the K Means related question), all you need to do is modify one function to solve the biggest problem that K Means algorithms face.\r\n",
        "\r\n",
        "####Questionaire:\r\n",
        "These are non-evaluative, but highly recommended to go through. Make sure you clearly know the answer to each of these concepts. The answers to all these questions are somewhere in this notebook, so if you find yourself unclear with a concept, go back up and find the answer!\r\n",
        "\r\n",
        "1. What characteristics of Unsupervised Learning does the KNN algorithm have?\r\n",
        "2. What is the advantage of the NumPy library?\r\n",
        "3. Similar to the bar graph depicting the number of crimes per district, can you write a piece of code to build a bar graph showing number of crimes per category? (like, Theft from Vehicle and so on)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfgscrGetNvq"
      },
      "source": [
        "# Exercises (Evaluative)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wmbJjTjmZhC"
      },
      "source": [
        "## 1. A Robust K Means Algorithmic system\r\n",
        "We learnt about the initialization problem in K Means. Many a times, a cluster is not assigned any datapoints. Let us try to solve it. \r\n",
        "\r\n",
        "All we need to do is, keep re-initializing the centroids, until some or the other datapoint lies closest to it, and the datapoint is assigned that cluster. You need to redefine the `initialize_centroids` function to implement this functionality. \r\n",
        "\r\n",
        "Then run the model again to see if your function works. (Even After reinitializing multiple times, there should be no empty cluster)!\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S83PO_tRPHAU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgEorXSexJU_"
      },
      "source": [
        "## 2. Regression using K Nearest Neighbors\r\n",
        "Just like classification, we'll be carrying out Regression. Instead of finding the majority class, our Job is to find the average of the values of K Nearest Neighbors.\r\n",
        "\r\n",
        "In this exercise, we will be finding out the most probable *Hour* of crime in a particular locality. You can use any cordinates as the reference. Use k as 100."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eH8S6po6ynfG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}