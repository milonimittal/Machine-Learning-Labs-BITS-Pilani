{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"12 Reinforcement Learning.ipynb","provenance":[],"authorship_tag":"ABX9TyMgUVgXDTNgY6eGQ/uGIngf"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"h-HWo8VX0YJT"},"source":["# 12 Reinforcement Learning \n","\n","\n","![](https://drive.google.com/uc?id=1e3fTjnZLdR5skojks4zIzuc_qjsFgJZo)\n","\n","Welcome to the 12th session in the Machine Learning practical series. In this session, we'll be deviating a little from Deep Learning, and studying about Reinforcement Learning, a slightly different field of Machine Learning, which uses a slightly different approach towards *learning*. Its a really interesting domain, and has lots of unexplored possibilities. So let's dive into an overview of what Reinforcement Learning is.\n","\n","Imagine a real life scenario, how you learnt social behaviors. When you were really young, you did not necessarily know how to behave, and simply explored everything. In that, you did a lot of wrong things, and even some good things. Whenever you do a good thing, there was some sort of *reward* that you used to get. For example, maybe you scored good marks in your test when you were young, and your parents praised your effort, maybe even got you a gift. This reward is meant to encourage you to study more, and keep up your grades at school. On the other hand, you may have done a lot of \"bad\" things as well, and by bad, we mean, socially inacceptable actions, like, maybe, you broke something in your house, hurt your sibling, trashed the house, or picked up a curse word from somewhere. And immediately, you were *punished*, or *penalized* for it, maybe you got a scolding. So in the future, a subtle fear of this penalty would  stop you from doing it again (and if not, atleast make you think of it!). A really long history of rewards and penalties shapes us into socially acceptable humans. Now, it need not be just parents who reward/punish us, it can be other entities too - like, the society in general, including your immediate friend circle, the immediate surroundings, police and the courts, etc. This method of *learning* through rewards and penalties is called reinforced learning. An agent learns through constant reinforcement of ideas and tasks. \n","\n","This is completely different from traditional Supervised Learning. In Supervised Learning, the learning entity is shown multiple examples of a target, which we wish that the entitity learns to identify. That's not how we learn. We don't learn not to break things at home by watching millions of examples of other kids not doing the same. So, reinforcement learning does not entail elarning through example, but learning through trial and error, and a series of rewards and penalties. Pretty interesting, right?\n","\n","So where is reinforcement learning used? Reinforcement Learning is used when we care about some result, without caring about what path is chosen to achieve the result. This result need not necessarily be  end goals, but also guidelines/goals that are to be followed throughout the learning process, like, what is good (reward-worthy) and what is bad (penalty worthy). The task is to maximise the reward, or minimize the penalty, or a mixture of both, depending on what the scenario is. So within these guidelines, on the basis of which rewards or penalties are given, our task is to learn to do something, and how we do it doesn't matter much. Let's take some real life examples of how reinforcement learning is used. \n","\n","In 2017, [DeepMind](https://deepmind.com/) developed a model called [AlphaGo Zero](https://deepmind.com/blog/article/alphago-zero-starting-scratch), which beat the world champion in the game of Go, which is a Chinese Game, considered to be one of the most difficult board games, because of the extremely large number of possible states that can be achieved in a game (about $3^{361}$ states in total, which is of the order of $10^{172}$ .Lets imagine what would be required to train a model that plays this game using a supervised learning approach. First of all it would simply not be possible to gather datasets of the scale of $10^{172}$. But to be fair, such an approach has been attempted, and was pretty successful too. DeepMind, the same company that developed AlphaGo Zero, developed [AlphaGo](https://deepmind.com/research/case-studies/alphago-the-story-so-far) in 2015, which used a supervised learning approach, and beat a professional Go player (which at the time, was a huge deal - the fact that an AI model could beat a human). The approach was simple - AlphaGo learnt the game by playing thousands of matches with amateur and professional players. This obviously wouldn't make it totally perfect, but would atleast make the model learn enough to pay decently well. \n","\n","AlphaGo Zero didn't use Supervised Learning, but used Reinforcement Learning. Instead of providing the model with a mechanism to learn features of a dataset, but with some guidelines of the game. And a mechanism to reward or penalize the model over its history of actions and the opponents actions (note: not just the last action is rewarded/penalized, but the entire history of actions matters in the end result. So rewards/penalities are determined by a series of actions). And AlphaGo Zero played millions of games against itself, making itself better each time, and reached a point where it beat the world champion in Go. Because of this, Google bought DeepMind for $600,000,000 (USD 600 Million).\n","\n","\n","Reinforcement Learning is really popular in applications that relate to understanding and developing the potential of Human-Computer Interactions, or human behavior in general. For example, DeepMind created this model (above) to gain insights about how humans might have evolved to walk the way that we do today, entirely from scratch. The only guidelines provided in this case was a physical form (having a torso having 2 legs), and the fact that the farther it can traverse, more the reward. \n","\n","\n","If you wish to gain an intuition about the entire learning process of a Reinforcement Learning model, here is a video of an AI agent try to learn how to park a car."]},{"cell_type":"code","metadata":{"id":"b5OK9NNy6HDI"},"source":["from IPython.display import YouTubeVideo\n","YouTubeVideo('VMp6pq6_QjI')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KTS1MeGUkUQc"},"source":["Video Games are perfect environments for Reinforcement Learning models. Which is what we are going to do today. A lot of work has been done in developing Video Game interfaces to incorporate AI that can be trained by us. So we'll be learning how to apply reinforcement learning on a video game environment, as a proxy to how reinforcement learning can be applied in the real world, and even as a way to make this session interesting. \n","\n","Let us first discuss briefly what reinforcement learning is all about. Before that, let us install and import all necessary libraries. We'll be using PyTorch to run our model."]},{"cell_type":"code","metadata":{"id":"26TSpT7hXwtX"},"source":["!pip install box2d-py >./tmp\n","!pip3 install gym[Box_2D] --upgrade>./tmp\n","!pip install pyvirtualdisplay >./tmp\n","!sudo apt-get install xvfb >./tmp"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BQ4cgsJdYHJv"},"source":["%matplotlib inline"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5yreuhUMlv-s"},"source":["import gym\n","import matplotlib\n","import matplotlib.pyplot as plt\n","import matplotlib.animation\n","from IPython.display import HTML\n","from pyvirtualdisplay import Display\n","\n","#display\n","display = Display(visible=0, size=(512, 512))\n","display.start()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8VKLGxakYueH"},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch import optim\n","from torch.distributions import Categorical\n","device='cpu'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BAAvzrXDniI2"},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm_notebook\n","from datetime import datetime\n","from collections import Counter, deque\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","import random\n","from collections import defaultdict, namedtuple, deque"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Kv0aPM1NnnZh"},"source":["## Reinforcement Learning with OpenAI Gym\n","\n","[OpenAI](https://gym.openai.com/) provides us with a collection of interactive environments to test reinforcement learning systems, including many video game environments, and other graphic based interfaces, some simple, and others complex!  \n","\n","In OpenAI's own terms - \n","\n","![](https://drive.google.com/uc?id=1PbpIEcjP0oapY8hgL--XI2DxqeVUvk0_)\n","\n","Since we're already somewhat comfortable in PyTorch, we'll be using that in this session.\n","\n","So, step number 1 is to create an environment where we'll be building our model. For the purpose of this session, we've picked a problem called the [Mountain Car](https://gym.openai.com/envs/MountainCar-v0/). The problem is simple:\n","\n",">The goal is to drive up the mountain on the right; however, the car's engine is not strong enough to scale the mountain in a single pass. Therefore, the only way to succeed is to drive back and forth to build up momentum.\n","\n","![](https://drive.google.com/uc?id=1NvAwZp1EhEgcmZKobtgtjHRYiSrtNf2J)"]},{"cell_type":"markdown","metadata":{"id":"i8XXAGBztKfX"},"source":["All we need to do to initialize this environment is one simple line of code as follows."]},{"cell_type":"code","metadata":{"id":"YwMiWBcLYKmJ"},"source":["env = gym.make('MountainCar-v0')\n","state = env.reset()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aforwszntR8T"},"source":["Before we start, let us talk about what goes into a reinforcement learning model. \n","\n","In Reinforcement Learning, we divide the universe into *agents* and *environments*. Agent is the entity that learns or interacts with other such agents. An agent performs within an environment, under some circumstances, and makes an *action* based on it. The action will lead it to reach some *state* of the environement (like, here, after some action (acceleration,etc), the car will reach some height). And each state comes with its own circumstances, to which the actor(agent) needs to adapt to, and come up with another suitable action from that point. \n","\n","Some actions will increase the *final reward*, while some may lead to failures. So we need to find an optimal path, through various states of the environement, that will lead to maximization of reward (in this case, reaching the topmost point).\n","\n","Meaning, any action that leads to higher altitude, results in higher reward. We may not need any penalty in this case, but you can come up with a case that leads to some sort of penalty. So we represent the net reward ($reward - penalty$) as the *score*. \n","\n","So, for each environment, there is a finite set of state that can be achieved, and for each agent, only a finite set of actions can be taken. \n","\n","Corresponding to an action taken in a state, there is some reward (and/or penalty) that is associated. Our job is to *optimize* this action-reward pair such that this reward is maximized. This is the crux of all reinforcement learning systems. \n","\n","Let us see how this is implemented in the case of the Mountain Car problem."]},{"cell_type":"markdown","metadata":{"id":"7fbl_UFb7w-9"},"source":["### Random Actions and States\n","\n","So at this point we have a car which really doesn't know how to reach the top of the right mountain, so it would be trying random actions. Let us see how it performs. \n"]},{"cell_type":"code","metadata":{"id":"goCWym3ItJje"},"source":["#this is a random action case\n","score = 0\n","t=0\n","frames = []\n","while True:\n","    action = np.random.choice([0,1,2]) #here, actions can be one (randomly chosen value) out of three values from 0,1 and 2. \n","                                        #See the source code to understand what it means\n","    \n","    state, reward, done, info = env.step(action) #the step function is the functin that changes the state of the car. See the source code\n","    t+=1\n","    score+=reward #we care about the total reward of all actions\n","    \n","    #append frame\n","    frames.append(env.render(mode = 'rgb_array')) #env.render returns a graphical image of the actions taking place. \n","                                                #Colab actually doesn't support dynamically changing popup windows, \n","                                                #so we're first storing it in a list, and then we'll make a small gif out of it\n","    if done: break #done is a boolean variable. Which represents whether or not we have reached the end of the prediction cycle. \n","\n","print(f'Done in {t} timsteps with score {score}.') #just some book keeping stats"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qEtLHb5zndyZ"},"source":["#this little cell of code converts our frames list to a HTML embedding that can be played as a video clip\n","#animate frames\n","patch = plt.imshow(frames[0])\n","animate = lambda i: patch.set_data(frames[i])\n","ani = matplotlib.animation.FuncAnimation(plt.gcf(), animate, frames=len(frames), interval = 1)\n","HTML(ani.to_jshtml())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5Nlozgnz4Si8"},"source":["In order to understand what exactly is happening here, we need to dive into the [source code](https://github.com/openai/gym/blob/38a1f630dc9815a567aaf299ae5844c8f8b9a6fa/gym/envs/classic_control/mountain_car.py).\n","\n","PS: if you're also a physics enthusiast, you'll enjoy reading the code."]},{"cell_type":"code","metadata":{"id":"Bv61CXni3vj2"},"source":["??gym.envs.classic_control.MountainCarEnv\n","#look for how actions are defined, how steps are taken and how scores are calculated."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_fzNKdzV8FjR"},"source":["We only have a crude prediction model, that randomly choses the action of the car at the very start. As OpenAI says, OpenAI Gym provides an environment, and its upto us to implement a model that can learn to do the specific task we need to do. "]},{"cell_type":"markdown","metadata":{"id":"JdZZHDCp7tUp"},"source":["### Creating an AI Model : Deep Reinforcement Learning\n","\n","So we need to build a model that can predict actions appropriately in our environment. There are many learning models that have been developed specifically for reinforcement learning, like Markov Chain based models, and Q learning. But recently there has emerged a new type of modeling technique in reinforcement learning - What if this model is a *Neural Network*? (surprise surprise!), because, why not? Afterall what can be better for prediction than Neural Nets?! This is the state of the art reinforcement learning technique as of now. So, in the spirit of trying to learn the best, let us focus on Neural Nets for this session, and how it can be integrated with Reinforcement Learning. \n","\n","The goal of using Neural Nets is two fold - \n","1. Not only Neural Nets are the best option when it comes to prediction, either in terms of regression or classification of values, but also\n","2. Other modeling techniques involve some mathematics, which we try to avoid in these sessions. We also know Neural Nets well, so its easier to understand as well. And then, we know that ultimately, there's no point in learning something in depth, which we would not use in the end.\n","\n","But before that, let us set up some additional things that are common to all learning models. That is, we need a mechanism to store the states and actions of the past. Why do we need this? Obviously because we need to keep track of all transitional parameters. But also because, as we mentioned, rewards need to be assigned based on the entire action history. This is one feature of reinforcement learning - its not just the last action that determines the final state, but the entire set of actions taken. \n","\n","So we define a class that can store these parameters. What will be these parameters? We define them below in a tuple that we call *Transition*, that will be a tuple of parameters representing any transition (between actions, states and rewards). "]},{"cell_type":"code","metadata":{"id":"E1Uwuyx1rO-u"},"source":["Transition = namedtuple('Transition', ('state', 'action', 'reward', 'next_state','done'))\n","\n","class Memory:\n","    def __init__(self, capacity, seed = 1412):\n","        self.capacity = capacity\n","        self.memory = deque(maxlen=capacity) #incase you only want to retain the last `capacity` number of states\n","        self.seed = random.seed(seed)\n","\n","    def add(self, *args):\n","        t = Transition(*args)\n","        self.memory.append(t)\n","\n","    def sample(self, batch_size):\n","        ts = random.sample(self.memory, batch_size)\n","        states = torch.from_numpy(np.vstack([t.state for t in ts])).float().to(device)\n","        actions = torch.from_numpy(np.vstack([t.action for t in ts])).float().to(device)\n","        rewards = torch.from_numpy(np.vstack([t.reward for t in ts])).float().to(device)\n","        next_states = torch.from_numpy(np.vstack([t.next_state for t in ts])).float().to(device)\n","        dones = torch.from_numpy(np.vstack([t.done for t in ts]).astype(np.uint8)).float().to(device)\n","        return(states,actions,rewards,next_states,dones) #the transition parameters\n","\n","    def __len__(self):\n","        return(len(self.memory))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QfPZBDHoFT5H"},"source":["Let us now build our main model - The Neural Network that learns how to predict! In Reinforcement Learning, it has been given a fancy name - Deep Q Networks. Thats because its just a Deep Learning variant of Q learning. So we'll understand what Q learning is basically about, as we proceed. \n"]},{"cell_type":"code","metadata":{"id":"5fHNsIGErfb0"},"source":["#This class is just a structure for storing different things, including the model, its hyperparameters, the optimizer, the memory_storage, etc\n","class DQNAgent:\n","    def __init__(self, state_size = 2, action_size = 3, replay_memory = None, seed = 1412,\n","        lr = 1e-3 / 4, bs = 64, nb_hidden = 256, clip = 1.,\n","        gamma=0.99, tau= 1e-3, update_interval = 5, update_times = 1, tpe = 200):\n","        \n","        self.state_size = state_size\n","        self.action_size = action_size\n","        self.seed = random.seed(seed)\n","        self.npseed = np.random.seed(seed)\n","        self.lr = lr\n","        self.bs = bs\n","        self.gamma = gamma\n","        self.update_interval = update_interval\n","        self.update_times = update_times\n","        self.tau = tau\n","        self.losses = []\n","        self.tpe = tpe\n","        self.clip = clip\n","\n","        #vanilla\n","        self.network_local = QNetwork(state_size, action_size, nb_hidden).to(device)\n","        self.network_target = QNetwork(state_size, action_size, nb_hidden).to(device)\n","        \n","        #dueling\n","#         self.network_local = DuelingNetwork(state_size, action_size, nb_hidden).to(device)\n","#         self.network_target = DuelingNetwork(state_size, action_size, nb_hidden).to(device)\n","        \n","        #optimizer. Adam optimizer, we even used this is CNNs\n","        self.optimizer = optim.Adam(self.network_local.parameters(), lr=self.lr)\n","\n","        # replay memory\n","        self.memory = replay_memory\n","        # count time steps\n","        self.t_step = 0\n","        \n","    def get_eps(self, i, eps_start = 1., eps_end = 0.001, eps_decay = 0.9):\n","        eps = max(eps_start * (eps_decay ** i), eps_end)\n","        return(eps)\n","    \n","    def step(self, state, action, reward, next_state, done):\n","        #add transition to replay memory\n","        self.memory.add(state, action, reward, next_state, done)\n","        \n","        #update target network\n","        self.soft_update(self.network_local, self.network_target)\n","#         self.hard_update(self.network_local, self.network_target)\n","        \n","        # learn every self.t_step\n","        self.t_step += 1\n","        if self.t_step % self.update_interval == 0:\n","            if len(self.memory) > self.bs:\n","                #vanilla\n","                for _ in range(self.update_times):\n","                    transitions = self.memory.sample(self.bs)\n","                    self.learn(transitions)\n","\n","    def act(self, state):\n","        eps = self.get_eps(int(self.t_step / self.tpe))\n","        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n","        self.network_local.eval()\n","        with torch.no_grad():\n","            action_values = self.network_local(state)\n","        self.network_local.train()\n","\n","        #epsilon greedy\n","        if random.random() > eps:\n","            return np.argmax(action_values.cpu().data.numpy())\n","        else:\n","            return random.choice(np.arange(self.action_size))\n","        \n","    def vanilla_loss(self,q_targets,q_expected):\n","        loss = F.mse_loss(q_expected,q_targets)\n","        return(loss)\n","        \n","    def learn(self, transitions, small_e = 1e-5):\n","        #vanilla\n","        states, actions, rewards, next_states, dones = transitions\n","        max_actions_next = self.network_local(next_states).detach().max(1)[1].unsqueeze(1)\n","        q_targets_next = self.network_target(next_states).detach().gather(1, max_actions_next.long())\n","\n","        #compute loss\n","        q_targets = rewards + (self.gamma * q_targets_next) * (1 - dones) #this is the crux of Q learning. So this is how Deep Learning and Q learning have been combined.\n","        q_expected = self.network_local(states).gather(1, actions.long())\n","        #vanilla\n","        loss = self.vanilla_loss(q_expected, q_targets)\n","        #append for reporting\n","        self.losses.append(loss)\n","        \n","        #backprop\n","        self.optimizer.zero_grad()\n","        loss.backward()\n","        if self.clip: torch.nn.utils.clip_grad_norm(self.network_local.parameters(), self.clip)\n","        self.optimizer.step()\n","      \n","    def hard_update(self, local_model, target_model):\n","        if self.t_step % 1/self.tau==0:\n","            target_model.load_state_dict(local_model.state_dict())\n","            \n","    def soft_update(self, local_model, target_model):\n","        \"\"\"Soft update model parameters.\n","        θ_target = τ*θ_local + (1 - τ)*θ_target\n","        \"\"\"\n","        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n","            target_param.data.copy_(self.tau*local_param.data + (1.0-self.tau)*target_param.data)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Bxn07sD8H-d0"},"source":["Now let us simply define our model, which is simply a Neural Network. For this we'll be using a simple Fully Connected, 3 Layer Neural Network."]},{"cell_type":"code","metadata":{"id":"CTdsIO2MseQb"},"source":["class QNetwork(nn.Module):\n","    def __init__(self, state_size, action_size, nb_hidden, seed=1412):\n","        super(QNetwork, self).__init__()\n","        self.seed = torch.manual_seed(seed)\n","        self.model = nn.Sequential(\n","            nn.Linear(state_size, nb_hidden),\n","            nn.ReLU(),\n","            nn.Linear(nb_hidden, nb_hidden),\n","            nn.ReLU(),\n","            nn.Linear(nb_hidden, action_size)\n","        )\n","    def forward(self, state):\n","        x = self.model(state)\n","        return(x)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-E-J8w71KTHO"},"source":["That's it. Now let us see how to train this model for our particular problem.\n","\n","Try changing the hyperparameters once you run this model using what has been provided to you."]},{"cell_type":"code","metadata":{"id":"46rqyrtQrsPT"},"source":["#initializing the environment, the model and memory states\n","\n","env = gym.make('MountainCar-v0') #reinitializing the environment\n","mem = Memory(capacity=int(1e3)) #always store `capacity` timesteps\n","a = DQNAgent(state_size = 2, action_size = 3, replay_memory = mem, seed = 1412,\n","        lr = 1e-3, #learning rate; try something between 1e-4 and 1e-3\n","        bs = 64, #batch size; try 64, 128, 256\n","        clip = 1., #gradient clipping\n","        nb_hidden = 64, #number of hidden units in the q network; try 64,128,256,512\n","        gamma = 0.99, #discount factor\n","        tau= 1e-3, #how fast we update the target network; try something between 1e-2 and 1e-3\n","        update_interval = 5,  #how often we update local network\n","        update_times = 1, #how many times you train the local network per update\n","        tpe = 200) #how many timesteps per episode"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jFBigRZpPJsY"},"source":["Let us train the model. You can read the DQNAgent class properly to understand (braodly) what's happening. You don't need to memorize any method per se. "]},{"cell_type":"code","metadata":{"id":"2msctIiLr4-G"},"source":["scores = []\n","scores_deque = deque(maxlen=100) \n","moving_scores = []\n","start_time = datetime.now()\n","\n","for i in tqdm_notebook(range(1000)): #training for 1000 epochs.\n","    if (i+1) % 100==0: print(f'Episode {i} Moving Average: {np.mean(scores_deque)}')\n","    state = env.reset()\n","    score = 0\n","    t=0\n","    while True:\n","        t+=1\n","        #select action\n","        action = a.act(state)  #there's a threshold based function that chooses the action based on the present state\n","\n","        #env step\n","        next_state, reward, done, info = env.step(action)   #inbuilt function. Nothing for us to worry about\n","        \n","        #You can engineer the reward to motive your agent even more\n","        reward_ = reward\n","\n","        #agent step\n","        a.step(state,action,reward_,next_state,done)\n","        \n","        #collect score\n","        score += reward\n","        \n","        #go to next state\n","        state = next_state   \n","        \n","        #break if done\n","        if done: break\n","            \n","    #book keeping\n","    scores.append(score)\n","    scores_deque.append(score)\n","    moving_scores.append(np.mean(scores_deque))\n","        \n","    #solved at -110; easy solve at -150\n","    if moving_scores[-1] > -110: \n","        print(f'Solved at Play {i}: {datetime.now() - start_time} Moving average: {moving_scores[-1]}')\n","        break"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Nw3gj_F2PZ-k"},"source":["On closer inspection, you'll notice there is not much difference between the broader methods of Deep Learning and Reinforcement Learning. There is some sort of connections between the parametric approach of deep learning and reinforcement learning, and the optimization method. There are hyperparameters that need to be optimized, and the reward is loosely related to the loss function. I stress on the word \"loose\" because in actual, theorists will tell you (correctly), the approaches are quite far apart. My goal of stating this point is, to point out the learning approach in all of AI, which has some common themes. \n","\n","\n","Let us plot the losses, and rewards"]},{"cell_type":"code","metadata":{"id":"IZSvBf-ds1IO"},"source":["plt.plot(scores)\n","plt.plot(moving_scores)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ULLQR0YnQ46h"},"source":["In the plot above, we've plotted the scores, which is nothing but the reward over the training cycle. The higher the better! So you can see, that around 600-700 epochs, the reward maximized. "]},{"cell_type":"code","metadata":{"id":"X035OjJKuWzN"},"source":["plt.plot(a.losses)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-v5coUUsS8Fy"},"source":["In this plot, we show the *losses* from the Neural Network. You can see, that after about 25000 batches, the loss is at its minimum. (Which is around the same time as the reward maximized). \n","\n","So, we may interpret that the model fits well around this point, and sort of overfits beyond that. So you may change the number of epochs too, while optimizing this model."]},{"cell_type":"markdown","metadata":{"id":"KT5xhsuTT-R4"},"source":["### Model Inference \n","\n","Well, we've trained the model, and interpreted it. But let's see it working! So here, we reinitialize the environment (bring the car to its initial state), and run our model on it. Its as simple as feedforwarding states through the network, "]},{"cell_type":"code","metadata":{"id":"sN2M_9dou1_S"},"source":["env = gym.make('MountainCar-v0')\n","state = env.reset()\n","score = 0\n","t=0\n","frames = []\n","for i in range(200):\n","    #only infer action; no training\n","    action = a.act(state)\n","    \n","    #env step\n","    state, reward, done, info = env.step(action)\n","    \n","    #book keeping\n","    t+=1\n","    score+=reward\n","    \n","    #break if done\n","    if done: break\n","    \n","    #append frame\n","    frames.append(env.render(mode = 'rgb_array'))\n","print(f'Done in {t} timsteps with score {score}.')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JFRacpmUURhr"},"source":["And finally, let's run the cell that we used to animate the results."]},{"cell_type":"code","metadata":{"id":"thKsPgxuu972"},"source":["patch = plt.imshow(frames[0])\n","animate = lambda i: patch.set_data(frames[i])\n","ani = matplotlib.animation.FuncAnimation(plt.gcf(), animate, frames=len(frames), interval = 1)\n","HTML(ani.to_jshtml())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R1mim1bLUWJ5"},"source":["That's definitely an improvement. You can optimize this model by changing hyperparameters, the model architecture, or the optimizer. But, the most significant change can come simply by optimizing the hyperparameters, without touching the model or optimizer. \n","\n","However, for those, who really want to experiment above and beyond, certainly do free to change the model architecture, optimizer, or anything else that you feel necessary."]},{"cell_type":"markdown","metadata":{"id":"X7iqz9UYU3me"},"source":["## Exercise \n","\n","This is a subjective exercise, meaning everyone may have different results. Your task is to optimize this model by changing different things (mostly hyperparameters, but again, you are free to experiment with other things). Try making this model work perfectly. Our ideal goal would be to make the car reach the top in one go! "]},{"cell_type":"code","metadata":{"id":"DjlRd6ExXWhF"},"source":[""],"execution_count":null,"outputs":[]}]}