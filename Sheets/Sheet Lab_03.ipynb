{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "03 SVD and PCA.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91WXEXkw-rwP"
      },
      "source": [
        "# Session 3: SVD and PCA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWleESoE-0cW"
      },
      "source": [
        "Welcome to Session 3 of the practical Machine Learning sessions. In this practical session, we'll understand and implement SVD and PCA, which stand for **Singlular Value Decomposition** and **Principal Component Analysis**. These two ideas are very similar, and have the same end goal. \r\n",
        "\r\n",
        "In this session, we'll only introduce ourselves to these concepts. The reason we won't go deep into these topics in this session itself, is because these topics have no practical application independently. They are used as a part of bigger models. So in this session we'll understand the concepts and intuition behind SVD and PCA, and later use it as a part of other Machine Learning projects. But yet, these topics deserve an entire session dedicated to themselves, because even though we cannot build a model using these topics, the idea behind them is quite important in Machine Learning, and its not totally self-intuitive. So the ideas do need a little bit of explanation. Also, we will still build interesting models using these concepts.\r\n",
        "\r\n",
        "As you know, these sessions are more practically-oriented than theoretically-oriented. So, we will not dive into most of the mathematics and theory behind the concepts, and only touch the information that is needed to build the code for our model. Our focus will be on understanding the functionality. You can find the exact innards of these concepts through a lot of online resources, and even the theory lectures of this course. However, this is a concept that does require a little bit of mathematics to intuitively understand the concept, and so if you don't get it at once, don't be discouraged! Go through it slowly and you will surely understand the concepts. \r\n",
        "\r\n",
        "So, Let us first understand what is the motivation behind these concepts, and then understand what SVD and PCA are. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAfFzw5NEg5b"
      },
      "source": [
        "## The Curse of Dimensionality\r\n",
        "\r\n",
        "Data, especially in recent times, often are of very high *dimensions*, or features. For example, satellite images sometimes contains thousands of bands of frequency spectrums, called *channels*, each of which is a separate feature. In Natural Language Processing (that is, the branch of AI that deals with Language understanding, for example, voice recognition (Siri, Google Assistant, Alexa), language to language translation, Sentiment Analysis (identifying hate speech, violence, etc)), words are represented as individual features. So if you wanted to model the entire English Language, you would be dealing with 1 million+ features. \r\n",
        "\r\n",
        "Infact, think of the amount of data that your phone generates. Modern phones have a lot of sensors, like Accelerometer, Gyroscope, Magnetometer, light sensor, biometric sensor (fingerprint scanner), GPS sensor, etc., all of which generate data continuously and independently, which is eventually transmitted to the company, which it uses to provide various services. Similarly, Aeroplanes generate hundreds of biliions of gigabytes of data every year, in the form of multiple sensors and manually fed data. The list goes on and on. \r\n",
        "\r\n",
        "Now, ideally, the goal of any model is to understand the data completely, or in other words, extract *all* the information that the data can provide to us, and the utopian solution to this is to build a model over all the data available, including all features.\r\n",
        "\r\n",
        "But, practically, its not an efficient method to use all the features of the data. \r\n",
        "\r\n",
        "1. It increases the time and space complexity of the problem. Hence, more resources are required. And this becomes a problem when we deal with huge amounts of data, (billions of data points, for example). (By the way, we call this *Big Data* in modern lingo). And even the most sophisticated computing system either are not capable of, or are pushed to the limits trying to handle such data. \r\n",
        "2. Most of the times, we do not even need all the features of the data. Many features are either useless, redundant, or provide very little information about the objective - so little, that even if we ignore the feature completely, there would not be a significant difference in the results. \r\n",
        "    \r\n",
        " For example, the health tracking app on your phone determines how many steps you walked in a day. The data from the biometric sensor is of no use for this application. However, an application that tracks your daily screen time would need data from the biometric scanner, the light sensor, etc, but on the other hand, would not require data from the gyroscope. Maybe the gyroscope is somewhat related to your usage of the phone, and may give some additional information about your screen time, but the light sensor and biometric sensor may give us enough information to accurately determine your screen time, and hence, we may not need the gyroscope information at all!\r\n",
        " \r\n",
        "So, if you pick any domain, you will, more or less, have access to a lot of types of data. But not all of the data is useful or feasible to use, and so, as Data Science practitioners, one of the most important tasks in any project is to understand what data is useful for your purpose, and which is not. This problem of having way too much information, is called the *Curse  of Dimensionality*.\r\n",
        "\r\n",
        "This leads us to the idea of ranking features by their importance. We would like to retain more important features and discard the less important features. Cognitively speaking, we can identify which features are important to our particular problem. For example, if we were to build the fitness tracking app of your phone, which can track how many steps you take, you would know that the data from the light sensor is more or less useless to build this feature. This is possible because we have an *understanding* that features which are related to the number of steps taken are more relative to this application, and others are not. \r\n",
        "\r\n",
        "But Machines do not understand these concepts, atleast not in today's world. They treat all features with equal importance. So how do we build a system that can rank features based on their importance?\r\n",
        "\r\n",
        "## Variance in Data: An intuition\r\n",
        "\r\n",
        "The idea is to rank features based on the *amount of information* each feature carries. In mathematics, the idea of the amount of information is represented by the *variance* of the value of a feature for all data points, or in other words, how spread out the values of a feature are.\r\n",
        "\r\n",
        "> If you're not familiar with the concept of variance, [here](https://www.youtube.com/watch?v=MRqtXL2WX2M) is a great source to give you an intuition. This video is based on the idea of *standard deviation*, which is very much similar to variance. In mathematics, variance is defined as the square of the standard deviation. So the idea is similar. A larger standard deviation also means a larger variance. However, note, that we are not concerned with the mathematical formula of variance. Here, by reference, we only are refering to the idea of \"spread\".\r\n",
        "\r\n",
        ">Note: Variance is a concept that is applied to a *distribution* of data, or in other words, a collection of data. The data that we deal with is also a distribution (collection of items). Hence for each feature, a separate distribution can be generated. The values of a particular feature of all data points will constitute the distribution. So each feature will also have its own variance.\r\n",
        "\r\n",
        "**The more the Variance, more is the information constituted in the feature.** The features with less variance do not provide us with a lot of information, and are thus, less relevant to us.\r\n",
        "\r\n",
        "To understand this concept more intuitively, have a look at this meme below. \r\n",
        "\r\n",
        "<figure><center>\r\n",
        "<img src='https://drive.google.com/uc?id=142b3dOpsaLbYlTIHdh4gSOxu0MFqa3B6' width='40%'>\r\n",
        "</center></figure>\r\n",
        "\r\n",
        "This is obviously meant as a joke, but it serves us as a great example. If you really wanted to differentiate between two species of animals, horses and crocodiles for example, you would collect data about these animals. The features could be - number of eyes, number of ears, weight, etc. As ridiculous as it may seem, even \"whether it is culpable for the death of Princess Diana\" is a feature. \r\n",
        "\r\n",
        "These features essentially have no *variance* among them. All horses have 2 eyes, and so do all crocodiles (atleast physically). No model in this world can differentiate between horses and crocodiles on the basis on number of eyes. This occurs due to the *lack of variance* in this feature. If you were to plot the number or eyes of 100 horses and 100 crocodiles, the distribution would not be spread out at all!\r\n",
        "\r\n",
        "Now consider the length of the ears of these two animals. The ears of a horse are of about 5 cm. Crocodiles on the other hand, do not have an external ear. So now you can easily differentiate between a horse and a crocodile now. This is because there exists variation in the data. And because of this, you can create two very distinct groups of animals.\r\n",
        "\r\n",
        "So now you see, among all the features, only one feature is sufficient to differentiate in this case. This is why extracting the more important features is important.\r\n",
        "\r\n",
        "The job of SVD and PCA algorithms is to find which features have the most variance, and hence carry the most information. Not only is it useful to identify the more important features, but also help us visualize data better. As you know, we can't imagine anything beyond 3 dimensions. So using SVD/PCA, we can represent almost all the information in just 3 dimensions, that would otherwise require, say, 100 dimensions. \r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "Lets learn about some details. We'll talk about SVD first, and then PCA. These 2 are anyways very similar concepts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IeR4t8Hu0j3S"
      },
      "source": [
        "## How SVD ranks features\r\n",
        "\r\n",
        "SVD refactorizes the input tensor (matrix) into a product of three matrices. Let us consider a matrix *A* of shape $m \\times n$. SVD breaks it down into three matrices $U,S$ and $V$, such that\r\n",
        "\r\n",
        "$A_{m \\times n} = U_{m \\times q} S_{q \\times q} V^T_{q\\times n}$\r\n",
        "\r\n",
        "\r\n",
        "where S is a eye Matrix (a matrix whose only diagonal values are non zero, and remaining are zeros), and $V^T$ refers to the inverse of matrix $V$. $q$ takes the value of $min(m,n)$. \r\n",
        "\r\n",
        "<figure><center>\r\n",
        "<img src='https://drive.google.com/uc?id=17fe47Y6cZuAedqNjvuYdz0igZGuJbrlr' width='70%'>\r\n",
        "</center></figure>\r\n",
        "\r\n",
        "Let us test this out. PyTorch provides us methods to calculate SVD and PCA. We'll use it for now, and later introduce you to other libraries that can carry out these operations. However, we prefer PyTorch because, as we mentioned, we'll be using SVD and PCA as part of a bigger project, like a Linear Regression Model, or a Logistic Regression Model, or a Neural Network, and in that case, PyTorch will help us avoid gradient calculations, and also provide us with *tensors*, that can handle data very well.\r\n",
        "\r\n",
        "We also import matplotlib at this point, which will help us visualize our tensors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLHzGjKl-SfU"
      },
      "source": [
        "import torch\r\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8VI305bmavQ"
      },
      "source": [
        "Let us generate a random tensor, of a random shape, lets say 10 x 6 (mxn)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANXLeEVCTmuW"
      },
      "source": [
        "A= torch.rand((10,6))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwPoFOOKnN3m"
      },
      "source": [
        "Let us perform SVD on it. If you google \"pytorch svd\", the first or second link will lead you to the documentation of SVD in PyTorch. [Here](https://pytorch.org/docs/stable/generated/torch.svd.html) is the documentation. In the example section, the syntax is mentioned. Let us follow the exact syntax."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pT6FYjAQmele"
      },
      "source": [
        "u,s,v = torch.svd(A)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3tXyqYhn74-"
      },
      "source": [
        "In PyTorch, all data containers are expected to be tensors. U, S and V too must be tensors. Let us verify that."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9ScVWDlWJiF"
      },
      "source": [
        "type(u), type(s), type(v)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VzEmRk-IoXiO"
      },
      "source": [
        "Let us also determine the shapes of these tensors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mizWWrXfA8y4"
      },
      "source": [
        "u.shape ,s.shape, v.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgFA-9r5orZq"
      },
      "source": [
        "$q$ is $min(m,n) = min(10,6) = 6$. \r\n",
        "\r\n",
        "So u is of shape $m \\times q = 10\\times6$ \r\n",
        "\r\n",
        "$s$ is supposed to a diagonal matrix. Instead, PyTorch directly provides us with the values of the diagonal elements. You can simply convert it to the diagonal matrix by `torch.diag(s)`.\r\n",
        "\r\n",
        "And finally $V^T$ is of shape $q\\times n =6\\times 6$ meaning, $V$ is of shape $6 \\times 6$ too.\r\n",
        "\r\n",
        "Let us verify if the relationship written above holds."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WF6cAGCGA9_d"
      },
      "source": [
        "u@torch.diag(s)@v.T #this is a tensor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWUhHu-WBcZm"
      },
      "source": [
        "A"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5avTnRX2sF3u"
      },
      "source": [
        "You can compare by a quick visual look. The two tensors are exactly identical.\r\n",
        "Using a randomly generated tensor may not be as intuitive. Let us try this with a real world example. Let us upload an image to this notebook. Let us work with a specific image. \r\n",
        "\r\n",
        "<figure><center>\r\n",
        "<img src='https://drive.google.com/uc?id=1cEmeBLncAbrbDparKHZ7hU_AXaRvJ_ql' width='50%'>\r\n",
        "</center></figure>\r\n",
        "\r\n",
        "This is a 1920x1080 pixel image. Pretty large! Let's download this image to our notebook. To save you time, we're using a publically shared google drive image, which can be downloaded into the local environment in Google Colab by using the `!gdown` statement!\r\n",
        "\r\n",
        "We also use the PIL library, which is a standard Python Image processing Library (ie, to read a jpg/png file). Also, in order to convert jpg images to PyTorch tensors, we'll need the PyTorch child library torchvision."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6O0H-FS9K-6"
      },
      "source": [
        "from PIL import Image\r\n",
        "import torchvision\r\n",
        "\r\n",
        "#let us also define functions that convert image to tensor and viceversa\r\n",
        "image2tensor = torchvision.transforms.ToTensor()\r\n",
        "tensor2image = torchvision.transforms.ToPILImage()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBc6FqwHB2cF"
      },
      "source": [
        "!gdown --id 1cEmeBLncAbrbDparKHZ7hU_AXaRvJ_ql >/.tmp\r\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YI_JMReLCdgM"
      },
      "source": [
        "img=Image.open('03 landscape.jpg')\r\n",
        "img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JK6iEQAZCqbA"
      },
      "source": [
        "tensor_landscape = image2tensor(img).float()\r\n",
        "tensor_landscape.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLW8u-aC99nm"
      },
      "source": [
        "Notice that this image has 3 channels (Red Green and Blue), each of which is a tensor of shape (1080 x 1920). In total, there are 3x1080x1920 pixels = 6,220,800 pixels (6.2 million pixels). Thats a lot of pixels. We often undervalue modern technology. Just a few decades ago, it would not have been possible to fit this image in a single computer, that occupied an entire room ([source](https://www.snopes.com/fact-check/computer-storage-1956/)).\r\n",
        "\r\n",
        "> Note: In Many other libraries, image tensors are usually of the shape (m,n,3) rather than (3,m,n). This is one speciality about PyTorch. This is because PyTorch does all operations from the right. For example, PCA/SVD is done on a 2D matrix. So PyTorch would only operate SVD/PCA on the right most 2 dimensions, and leave the rest dimensions untouched. As you will see, this is not only efficient (doesn't require extra lines of code to exclude certain dimensions), but also is desirable for visualization as well as ease of operation of large amounts of data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRrXNtHoADv6"
      },
      "source": [
        "## So Now let us apply SVD to this image.\r\n",
        "same method as above!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNjbKeyVJ-xS"
      },
      "source": [
        "u,s,v = torch.svd(tensor_landscape)\r\n",
        "u.shape, s.shape, v.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MiHsSRS6AiHh"
      },
      "source": [
        "Notice the extra dimension of 3 in these tensors. This is because of the 3 channels in the image. PyTorch has applied SVD on all 3 2-D matrices independently, and kept all dimensions separate.\r\n",
        "\r\n",
        "Let us verify that we can reobtain the image from these vectors. We have defined a function that can take these decomposed tensors (u,s,v) and return a tensor which represents the original tensors. You can ignore all the new PyTorch functions used in this function. We do this because the `@` operator does not know how to handle batches. So we have to use inbuilt pytorch functions. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4oxI6awkBNYT"
      },
      "source": [
        "def decomposed2tensor(u,s,v): return torch.matmul(torch.matmul(u,torch.diag_embed(s)),torch.transpose(v,-1,-2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UI0v6sQcDT-D"
      },
      "source": [
        "tensor = decomposed2tensor(u,s,v)\r\n",
        "tensor2image(tensor).resize((800,400))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_o5tkstGl4N"
      },
      "source": [
        "We have recovered the same image!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1nikYpGG4kt"
      },
      "source": [
        "## So now lets choose a subset of features.\r\n",
        "As you would remember from the MNIST example from the Logistic Regression session, each pixel in this image is a separate feature. Here too, each pixel is a separate feature. But there are way too many features here - 6,220,800 features.\r\n",
        "\r\n",
        "How do we choose the most important features?\r\n",
        "\r\n",
        "The $S$ tensor is the key to our answer. $S$ gives the variance of each new feature in descending order. Hence, it also represents features in the descending order of the amount of information the features carry. \r\n",
        "\r\n",
        "Let us see this for ourselves! \r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwJhx3s4JJsk"
      },
      "source": [
        "plt.plot(s[0])\r\n",
        "plt.xlabel('feature number')\r\n",
        "plt.ylabel('amount of information')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S73RavT5K-3s"
      },
      "source": [
        "As you can see, $S$ is a constantly decreasing tensor. The first value corresponds to the feature with the highest variance. You can also notice - the first 50 or so features carry most of the information. Then why would we need the remaning 1030 features? \r\n",
        "\r\n",
        "> Note: This method of determination of number of useful features is called the elbow selection method. Wherever there is a steep bend in the amount of information is a good selection of number of crucial features. However, this is not a hard and fast rule, and trial and error is the only absolute method. It depends on how much information you're willing to throw away.\r\n",
        "\r\n",
        "By the way, internally, $USV^T$ is not exactly same as $A$, even though the final values are the same. Actually, U, S and V jumble up all the features of A in such a manner, that the feature with the most variance (can be a linear combination on many original features) emerges at the first position, and the one with the least variance comes at the end. \r\n",
        "\r\n",
        "Let us now choose the first 50 most important features from this photo.\r\n",
        "\r\n",
        "\r\n",
        "Remember, A is decomposed as \r\n",
        "\r\n",
        "$A_{m \\times n} = U_{m \\times q} S_{q \\times q} V^T_{q\\times n}$\r\n",
        "\r\n",
        "\r\n",
        "Out of q features, if you were to choose 50 features you would choose the first 50 elements of $S$. Correspondingly you would choose the first 50 elements of the columns of $U$ and first 50 rows of $V^T$, in order to make the matrix operations compatible. Here is a great visualization of the concepts.\r\n",
        "\r\n",
        "<figure><center>\r\n",
        "<img src='https://drive.google.com/uc?id=1cYCEqI-9Bbl8-wC6cTkdc0HSCKC4MN2t' width='40%'>\r\n",
        "</center></figure>\r\n",
        "\r\n",
        "Or mathematically,\r\n",
        "\r\n",
        "$A_{m \\times n} = U_{m \\times 50} S_{50 \\times 50} V^T_{50\\times n}$\r\n",
        "\r\n",
        "The most beautiful thing about this equation is that, the output of the product of these vector is still the original size (ie, 1080x1920 pixels)\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7IwuiSxAD0o1"
      },
      "source": [
        "u1,s1,v1 = u[:,:,:50] , s[:,:50] , v[:,:,:50]\r\n",
        "u1.shape, s1.shape, v1.shape "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulpX-1vuPJRb"
      },
      "source": [
        "Let us Reconstruct the original image from these vectors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyJrh6gvDYM-"
      },
      "source": [
        "tensor = decomposed2tensor(u1,s1,v1)\r\n",
        "img=tensor2image(tensor)\r\n",
        "img.resize((800,400))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-AcHGJXRGQD"
      },
      "source": [
        "The image quality has slightly gone down, But, the image is still understandable. All the while, having reduced its size by a lot. \r\n",
        "\r\n",
        "The original Image was 3x1920x1080 pixel values = 6,220,800 pixels\r\n",
        "This image, made from decomposed matrices, have a size of only 450150 values (3x1080x50 + 3x50 + 3x1920x50), which is over 13 times smaller than the size of the file. If the original file was 5MB, this file would be less than 400 KiloBytes. \r\n",
        "\r\n",
        "This is how WhatsApp transfers images. It does not transfer the image in its original quality, but *compresses* the image, using a similar algorithm. The quality is not compromised much, but the size reduces dramatically. \r\n",
        "\r\n",
        "How much variance (information) has been used to generate this image?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hs5BpVm5RuCa"
      },
      "source": [
        "print((torch.sum(s[:,:50])/torch.sum(s)), '% of the information has been used')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYR-lNohU_QJ"
      },
      "source": [
        "Meaning, almost the first 50 features contain 50% more information than all remaining 1030 features combined."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X46pKj5bV4w8"
      },
      "source": [
        "Obviously, SVD throws away certain information. So we can't expect the quality of the image to be the same as the original, and at the same time, the memory efficiency of the data to reduce. You always need to do a tradeoff. But SVD is a great tool when memory is an issue, and at the same time, *some* tradeoff in the quality of data is acceptable. For example, we can still accept a Machine Learning model to identify that this is a picture of a lake and mountains, even though we've reduced the size by over 13 times. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nsTCqiJXX46"
      },
      "source": [
        "___\r\n",
        "\r\n",
        "For the sake of simplicity, let us redefine a function, so that we dont have to manually slice tensors according to the number of features to be selected. To this you can pass the decomposed tensors, as well as (optionally) how many features to choose. If nothing is passed to `k`,all the features are chosen.\r\n",
        "\r\n",
        "You can also pass in any types of data, not necessarily a single image, but a batch of mutiple images (both multichanneled and single channeled (such as Black and White images). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qWmzlhiERlm"
      },
      "source": [
        "def decomposed2tensor(u,s,v,k=None):\r\n",
        "    if k is None: k=s.shape[-1] + 1 \r\n",
        "    u1,s1,v1 = u[...,:k] , s[...,:k] , v[...,:k]\r\n",
        "    return torch.matmul(torch.matmul(u1,torch.diag_embed(s1)),torch.transpose(v1,-1,-2))\r\n",
        "\r\n",
        "\r\n",
        "#does it work?\r\n",
        "decomposed2tensor(u,s,v,k=10).shape , decomposed2tensor(u,s,v).shape "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7V9SnyDaCrR"
      },
      "source": [
        "### PCA\r\n",
        "\r\n",
        "PCA is an optimized version of SVD. \r\n",
        "* It normalizes the data before decomposing it\r\n",
        "* It only calculates certain number of features, and not all possible features, thus speeding up the process of calculations. In PyTorch, this value is $min(6,m,n)$, though it can be adjusted by us.\r\n",
        "\r\n",
        "[Here](https://pytorch.org/docs/stable/generated/torch.pca_lowrank.html) is the documentation of the function.\r\n",
        "\r\n",
        "If you remove the normalization, and set the number of features to be calculated the same as SVD, then this algorithm will basically work like SVD. Hence, you see, PCA and SVD are not very different algorithms.\r\n",
        "\r\n",
        "For this example, we will not normalize the data, because normalization usually does not work well with images. Just to demonstrate, we first show you the result of PCA *with* normalization. \r\n",
        "\r\n",
        "However, for other types of data, normalization gives better, more stable results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swQhB-QRaCBn"
      },
      "source": [
        "u,s,v = torch.pca_lowrank(tensor_landscape) # with normalization\r\n",
        "u.shape, s.shape, v.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RgOWrR41YGFs"
      },
      "source": [
        "tensor=decomposed2tensor(u,s,v)\r\n",
        "tensor2image(tensor).resize((500,500))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZ8pa-VzcT_t"
      },
      "source": [
        "Clearly, normalization creates a lot of noise in Images. To avoid normalization, set the `center` parameter as `False`. We will also choose 50 features, just as before, using the `q` parameter. (Read documentation to understand syntax)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8VRg0z0bgYp"
      },
      "source": [
        "u,s,v = torch.pca_lowrank(tensor_landscape,center=False,q=50)\r\n",
        "tensor=decomposed2tensor(u,s,v)\r\n",
        "tensor2image(tensor).resize((800,400))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQU12n7Lc7Sd"
      },
      "source": [
        "This is absolutely the same as the result we received from SVD."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_IzLBxKhl9yD"
      },
      "source": [
        "## Can we Apply SVD/PCA through other libraries too? (Optional)\r\n",
        "Yes we can! Why just stick to PyTorch? In Many cases, other libraries may be more appropriate to use!\r\n",
        "\r\n",
        "1. ### **Using NumPy**\r\n",
        "\r\n",
        " NumPy provides us with a method to calculate the decomposed matrices (U,S,V). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJsnhxSJnXK1"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubuyzZBNna9E"
      },
      "source": [
        "Let us begin by creating a random matrix, just like we did in the beginning of this notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LViBrQaPnZiF"
      },
      "source": [
        "array = np.random.randn(10, 6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2wuEb86oHfP"
      },
      "source": [
        "Now let us calucalte the decomposed matrices. Note: that np.linalg.svd directly returns $V^T$, and not $V$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVRd_eI5oG4d"
      },
      "source": [
        "u,s,vT = np.linalg.svd(array,full_matrices=False) \r\n",
        "u.shape, s.shape, vT.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0M5eeHWTomL1"
      },
      "source": [
        "Can we reconstruct `array` from these matrices?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFYkVgTPnZe2"
      },
      "source": [
        "pred = u @ np.diag(s) @ vT\r\n",
        "pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ff8G1w4lpSVK"
      },
      "source": [
        "array"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZF11t2vphEX"
      },
      "source": [
        "You can compare that these two matrices are the same. Again you can simply slice the decomposed matrices to obtain the more important features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZia4D-ypr71"
      },
      "source": [
        "### 2. **PCA Using sklearn** \r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2FULrzxp2SC"
      },
      "source": [
        "from sklearn.decomposition import PCA"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWj_iQxZp3dJ"
      },
      "source": [
        "pca = PCA(n_components=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76ZW59ijp8NI"
      },
      "source": [
        "sklearn too expects numpy arrays as input."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jnGxNrBqS4C"
      },
      "source": [
        "pca.fit(array);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTv933Uyr7-g"
      },
      "source": [
        "And then you can access the values of each of these matrices separately."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWuWvXJwqYnU"
      },
      "source": [
        "pca.singular_values_ # S values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jhw8NFr_sPTH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTbHwOI-eHBk"
      },
      "source": [
        "## Review \r\n",
        "There you have it! You have implemented the PCA and the SVD algorithm. Congratulations!\r\n",
        "Below we've given some review questions for you to try out. But before that, there are a few tips and review points, that you should go through, think over, and make sure you understand each of them. These points are the most important points of this entire session - and not surprisingly, these are the concepts and ideas that will actually help you be a good Machine Learning practitioner.\r\n",
        "\r\n",
        "* Its natural you would be feeling slightly overwhelmed by all the mathematics and concepts we learned above. But in reality, there are entire books written just on the mathematics of these algorithms. We've barely scratched the surface, and to be honest, thats all that we need to know to get what we need. \r\n",
        "\r\n",
        " Don't try to memorize the formulae, the concepts are enough to know! Regarding the code, you can always come back, and refer to the structure of the code, and even copy paste it to your own application. Programming is all about how to get the most out of the resources available. You should know the art of how to write, rather than what to write!\r\n",
        "\r\n",
        "* As Data Science Practitioners, we cannot completely avoid the math and the stats. However, in practical Machine Learning, a lot can be avoided (unless you're pursuing research, in which case, math is essential!). That being said, don't be afraid of the math. Patience is key!\r\n",
        "\r\n",
        "## Review Questions:\r\n",
        "These are non-evaluative, but highly recommended to go through. Make sure you clearly know the answer to each of these concepts. The answers to all these questions are somewhere in this notebook, so if you find yourself unclear with a concept, go back up and find the answer!\r\n",
        "\r\n",
        "1. Mathematically, how is the concept of *information* represented in data?\r\n",
        "2. What does the tensor $S$ represent in SVD (or PCA)?\r\n",
        "3. Why do we need to rank the importance of data. Can you give an example from outide the examples above?\r\n",
        "4. When is dimensionality reduction suitable?\r\n",
        "5. How do we determine how many features are the optimal number of features to use?\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10Wx7vkWtdLu"
      },
      "source": [
        "# Exercise (Evaluative):\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kxUdsEQtuqa"
      },
      "source": [
        "In Lab02: Logistic Regression, we built a MNIST Digit classifier. We acheived an accuracy of about 88% between 3's and 5's. Let us now further apply SVD to the dataset and see how it affects the performance of the mdel.\r\n",
        "To simulate this, we will apply reduce the size of all images in our dataset by selecting a few important *features* from the images.\r\n",
        "\r\n",
        "We provide you with a structure of the code we used before. You need to modify this code to apply PCA to the dataset.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8ALV3eeIW16"
      },
      "source": [
        "#necessary imports\r\n",
        "from torch.utils.data import DataLoader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0Qzj6yxIDXH"
      },
      "source": [
        "#necessary definitions\r\n",
        "\r\n",
        "def convert_PIL_to_tensors(images):\r\n",
        "    images=list(images) #to make sure we can index the collection of images properly. Because of this, the input to this function need\r\n",
        "                        #not necessarily be a list, but can a set, tuple, generator or even a dictionary\r\n",
        "    return torch.stack(list(map(torchvision.transforms.ToTensor(),images))).float()\r\n",
        "\r\n",
        "def init_params(size, requires_grad=True): return (torch.randn(size)).requires_grad_()\r\n",
        "\r\n",
        "def logistic_regression_model(x): \r\n",
        "    return torch.sigmoid(x@weights + bias)\r\n",
        "\r\n",
        "def binary_classification_loss(preds,targets):\r\n",
        "    assert len(preds)==len(targets)\r\n",
        "    return torch.where(targets==1,1-preds,preds).mean()\r\n",
        "\r\n",
        "def calc_grad(x_batch,y_batch,model): \r\n",
        "    preds=model(x_batch)\r\n",
        "    loss=binary_classification_loss(preds,y_batch)\r\n",
        "    loss.backward()\r\n",
        "\r\n",
        "def train_epoch(model,lr,params):\r\n",
        "    for x_batch,y_batch in dl: \r\n",
        "        calc_grad(x_batch,y_batch,model)\r\n",
        "        for p in params:\r\n",
        "            p.data -= lr*p.grad \r\n",
        "            p.grad.zero_()\r\n",
        "\r\n",
        "def batch_accuracy(preds,y_batch):\r\n",
        "    return ((preds>=0.5)==y_batch).float().mean()\r\n",
        "\r\n",
        "def validate_epoch(model):\r\n",
        "    accs=[batch_accuracy(model(x_batch),y_batch) for x_batch,y_batch in valid_dl]\r\n",
        "    return torch.stack(accs).mean().item()\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b92P4aY_E2Rh"
      },
      "source": [
        "mnist_train_ds = torchvision.datasets.MNIST(root='',download=True)\r\n",
        "mnist_test_ds  = torchvision.datasets.MNIST(root='',train = False, download=True)\r\n",
        "len(mnist_train_ds), len(mnist_test_ds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3OyVWS9E5gU"
      },
      "source": [
        "threes_ds = [i for i in mnist_train_ds if i[1]==3]\r\n",
        "fives_ds  = [i for i in mnist_train_ds if i[1]==5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlgCEjmGE9fw"
      },
      "source": [
        "threes_test_ds = [i for i in mnist_test_ds if i[1]==3]\r\n",
        "fives_test_ds  = [i for i in mnist_test_ds if i[1]==5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eiXcUdHpFF0K"
      },
      "source": [
        "x_threes=convert_PIL_to_tensors([i[0] for i in threes_ds]).view(-1,28*28)\r\n",
        "x_fives =convert_PIL_to_tensors([i[0] for i in fives_ds]).view(-1,28*28)\r\n",
        "\r\n",
        "x_dataset = torch.cat((x_threes,x_fives))\r\n",
        "y_dataset = torch.stack([torch.tensor(1.)]*len(x_threes) + [torch.tensor(0.)]*len(x_fives))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4Cj-S5PFJro"
      },
      "source": [
        "x_threes_test=convert_PIL_to_tensors([i[0] for i in threes_test_ds]).view(-1,28*28)\r\n",
        "x_fives_test =convert_PIL_to_tensors([i[0] for i in fives_test_ds]).view(-1,28*28)\r\n",
        "\r\n",
        "x_test_dataset = torch.cat((x_threes_test,x_fives_test))\r\n",
        "y_test_dataset = torch.stack([torch.tensor(1.)]*len(x_threes_test) + [torch.tensor(0.)]*len(x_fives_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edks4DU8It3y"
      },
      "source": [
        "At this point, when we trained our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFwDtUubFs_6"
      },
      "source": [
        "weights=init_params((28*28,1))\r\n",
        "bias=init_params(1)\r\n",
        "params=weights,bias\r\n",
        "\r\n",
        "train_dset=list(zip(x_dataset,y_dataset))\r\n",
        "valid_dset=list(zip(x_test_dataset,y_test_dataset))\r\n",
        "dl=DataLoader(train_dset,batch_size=16)\r\n",
        "valid_dl=DataLoader(valid_dset,batch_size=32)\r\n",
        "\r\n",
        "for _ in range(100):\r\n",
        "    train_epoch(logistic_regression_model,0.25,params)\r\n",
        "    print(validate_epoch(logistic_regression_model),end=' ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6o1j1Y2MKVL8"
      },
      "source": [
        "Now, apply svd to the train and test datasets. We apply SVD only to the x datasets , ie, only the `x_dataset` and the `x_test_dataset`. Determine the appropriate number of features through the training dataset. Train this dataset.\r\n",
        "\r\n",
        "Vary the number of features and record the change in final testing accuracy. Plot a number-of-features(k) vs accuracy plot for a few k values.\r\n",
        "\r\n",
        "You may use some of the functions defined earlier in this notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sx2VFYSVNuEM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}